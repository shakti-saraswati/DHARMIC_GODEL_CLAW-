"""
Mathematical Auditor (MVA) - DGC Core Component
AIKAGRYA Rigorous Mathematical Verification for DHARMIC_GODEL_CLAW

Provides formal verification of:
- R_V metric derivations and implementations
- Statistical methods (Cohen's d, p-values, confidence intervals)
- Causal inference claims
- Transformer circuit mathematics

Usage:
    from src.core.math_auditor import MathAuditor
    
    auditor = MathAuditor(telos="rigor-before-reach")
    report = auditor.audit_rv_claims(repo_path="~/mech-interp-latent-lab-phase1")

Created: 2026-02-05
Author: DHARMIC CLAW
Telos: rigor-before-reach
"""

import ast
import inspect
import json
import math
import re
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
import numpy as np


class VerificationStatus(Enum):
    """Status of a mathematical verification claim."""
    VALIDATED = "validated"
    CONCERN = "concern"
    REJECTED = "rejected"
    PENDING = "pending"
    INCONCLUSIVE = "inconclusive"


class AuditPriority(Enum):
    """Priority levels for audit findings."""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


@dataclass
class VerificationFinding:
    """A single finding from mathematical verification."""
    claim: str
    status: VerificationStatus
    confidence: float  # 0.0 to 1.0
    priority: AuditPriority
    details: str = ""
    recommendations: List[str] = field(default_factory=list)
    mathematical_proof: Optional[str] = None
    code_reference: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "claim": self.claim,
            "status": self.status.value,
            "confidence": f"{self.confidence:.2%}",
            "priority": self.priority.value,
            "details": self.details,
            "recommendations": self.recommendations,
            "mathematical_proof": self.mathematical_proof,
            "code_reference": self.code_reference,
        }


@dataclass
class FormalVerificationReport:
    """Complete formal verification report for AIKAGRYA research."""
    title: str
    summary: str
    overall_status: VerificationStatus
    overall_confidence: float
    findings: List[VerificationFinding] = field(default_factory=list)
    critical_issues: int = 0
    warnings: int = 0
    validated_claims: int = 0
    
    def add_finding(self, finding: VerificationFinding):
        self.findings.append(finding)
        if finding.status == VerificationStatus.REJECTED:
            self.critical_issues += 1
        elif finding.status == VerificationStatus.CONCERN:
            self.warnings += 1
        elif finding.status == VerificationStatus.VALIDATED:
            self.validated_claims += 1
    
    def to_markdown(self) -> str:
        """Generate formatted markdown report."""
        lines = [
            f"# {self.title}",
            "",
            f"**Overall Status**: {self.overall_status.value.upper()}",
            f"**Overall Confidence**: {self.overall_confidence:.1%}",
            f"**Critical Issues**: {self.critical_issues}",
            f"**Warnings**: {self.warnings}",
            f"**Validated Claims**: {self.validated_claims}",
            "",
            "## Executive Summary",
            "",
            self.summary,
            "",
            "## Findings",
            "",
        ]
        
        for i, finding in enumerate(self.findings, 1):
            status_emoji = {
                VerificationStatus.VALIDATED: "‚úÖ",
                VerificationStatus.CONCERN: "‚ö†Ô∏è",
                VerificationStatus.REJECTED: "üö®",
                VerificationStatus.PENDING: "‚è≥",
                VerificationStatus.INCONCLUSIVE: "‚ùì",
            }.get(finding.status, "‚ùì")
            
            lines.extend([
                f"### {i}. {finding.claim}",
                "",
                f"**Status**: {status_emoji} {finding.status.value.upper()}",
                f"**Confidence**: {finding.confidence:.1%}",
                f"**Priority**: {finding.priority.value.upper()}",
                "",
                finding.details,
                "",
            ])
            
            if finding.mathematical_proof:
                lines.extend([
                    "**Mathematical Proof**:",
                    "```",
                    finding.mathematical_proof,
                    "```",
                    "",
                ])
            
            if finding.recommendations:
                lines.extend([
                    "**Recommendations**:",
                    "",
                ])
                for rec in finding.recommendations:
                    lines.append(f"- {rec}")
                lines.append("")
        
        lines.extend([
            "---",
            "",
            "*Generated by Mathematical Verification Agent (MVA)*",
            "*AIKAGRYA Rigorous Verification Protocol*",
        ])
        
        return "\n".join(lines)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "title": self.title,
            "summary": self.summary,
            "overall_status": self.overall_status.value,
            "overall_confidence": self.overall_confidence,
            "statistics": {
                "critical_issues": self.critical_issues,
                "warnings": self.warnings,
                "validated_claims": self.validated_claims,
                "total_findings": len(self.findings),
            },
            "findings": [f.to_dict() for f in self.findings],
        }


class MathAuditor:
    """
    Mathematical Verification Agent for AIKAGRYA research rigor.
    
    Ensures all mathematical claims, statistical methods, and causal
    inferences are formally verified before publication.
    """
    
    def __init__(self, telos: str = "rigor-before-reach", debug: bool = False):
        """
        Initialize the Mathematical Auditor.
        
        Args:
            telos: The orienting purpose (default: rigor-before-reach)
            debug: Enable debug logging
        """
        self.telos = telos
        self.debug = debug
        self._audit_log: List[Dict[str, Any]] = []
        
    def _log(self, message: str, level: str = "info"):
        """Internal logging."""
        if self.debug or level in ["error", "critical"]:
            print(f"[MVA:{level.upper()}] {message}")
    
    # =========================================================================
    # R_V METRIC VERIFICATION
    # =========================================================================
    
    def verify_rv_definition(self) -> VerificationFinding:
        """
        Verify the mathematical definition of R_V is sound.
        
        R_V = det(Cov(V_recursive)) / det(Cov(V_baseline))
        """
        proof = """
        R_V Definition Verification:
        
        Given:
        - V_recursive: Value vectors from recursive prompts
        - V_baseline: Value vectors from baseline prompts
        - Cov(X) = E[(X - Œº)(X - Œº)·µÄ]: Covariance matrix
        - det(Œ£): Determinant (product of eigenvalues)
        
        R_V = det(Cov(V_recursive)) / det(Cov(V_baseline))
        
        Geometric Interpretation:
        - det(Œ£) ‚àù Volume of confidence ellipsoid
        - R_V < 1 ‚üπ Vol_recursive < Vol_baseline
        - This represents geometric contraction in value space
        
        Eigenvalue Form:
        - Œ£ = QŒõQ·µÄ (spectral decomposition)
        - det(Œ£) = ‚àèŒª·µ¢ (product of eigenvalues)
        - R_V = ‚àè(Œª·µ¢_recursive) / ‚àè(Œª·µ¢_baseline)
        
        Therefore: R_V < 1.0 iff recursive prompts produce value vectors
        with smaller spread (contraction) compared to baseline.
        """
        
        return VerificationFinding(
            claim="R_V = det(Cov(V_recursive)) / det(Cov(V_baseline)) is geometrically valid",
            status=VerificationStatus.VALIDATED,
            confidence=0.99,
            priority=AuditPriority.CRITICAL,
            details="Mathematical definition is sound. The ratio of determinants correctly measures "
                    "relative volume of confidence ellipsoids. R_V < 1.0 implies contraction.",
            mathematical_proof=proof,
            recommendations=[
                "Use log-determinant for numerical stability: log R_V = log det(Œ£_rec) - log det(Œ£_base)",
                "Add regularization if covariance matrices are near-singular",
            ],
        )
    
    def verify_covariance_computation(self) -> VerificationFinding:
        """Verify covariance matrix computation is correct."""
        proof = """
        Covariance Matrix Verification:
        
        Unbiased Estimator (correct):
        Cov(X) = 1/(n-1) Œ£·µ¢ (x·µ¢ - xÃÑ)(x·µ¢ - xÃÑ)·µÄ
        
        Biased Estimator (incorrect for small n):
        Cov(X) = 1/n Œ£·µ¢ (x·µ¢ - xÃÑ)(x·µ¢ - xÃÑ)·µÄ
        
        Properties:
        1. Symmetric: Cov(X) = Cov(X)·µÄ ‚úì
        2. Positive semi-definite: v·µÄCov(X)v ‚â• 0 for all v ‚úì
        3. det(Cov(X)) ‚â• 0 ‚úì
        
        The unbiased estimator with (n-1) denominator is required for
        sample covariance to be an unbiased estimator of population covariance.
        """
        
        return VerificationFinding(
            claim="Covariance matrix uses unbiased estimator (1/(n-1))",
            status=VerificationStatus.CONCERN,
            confidence=0.75,
            priority=AuditPriority.CRITICAL,
            details="Must verify code uses (n-1) denominator, not n. Common error in implementations.",
            mathematical_proof=proof,
            recommendations=[
                "Audit src/metrics/rv.py for np.cov usage (uses n-1 by default)",
                "If using manual implementation, verify denominator is (n-1)",
                "Add unit test with known covariance matrix",
            ],
        )
    
    def verify_sample_size_requirements(
        self, 
        n_samples: int, 
        n_dimensions: int
    ) -> VerificationFinding:
        """
        Verify sample size is adequate for covariance estimation.
        
        Rule: n >> d for stable estimation
        """
        ratio = n_samples / n_dimensions
        
        if ratio < 1:
            status = VerificationStatus.REJECTED
            confidence = 0.95
            details = (f"CRITICAL: n ({n_samples}) < d ({n_dimensions}). Covariance matrix is "
                       f"guaranteed to be singular. Estimation impossible.")
            recommendations = [
                f"Increase samples to at least {n_dimensions * 10}",
                "Use PCA dimensionality reduction first",
                "Apply Ledoit-Wolf shrinkage estimator",
            ]
        elif ratio < 10:
            status = VerificationStatus.CONCERN
            confidence = 0.80
            details = (f"WARNING: n/d ratio = {ratio:.1f} < 10. Covariance estimation may be unstable. "
                       f"Determinants will have high variance.")
            recommendations = [
                f"Recommend n ‚â• {n_dimensions * 10} for stable estimation",
                "Use regularized covariance estimator",
                "Bootstrap confidence intervals for R_V",
            ]
        else:
            status = VerificationStatus.VALIDATED
            confidence = 0.90
            details = f"OK: n/d ratio = {ratio:.1f} ‚â• 10. Sample size adequate for stable covariance estimation."
            recommendations = []
        
        proof = f"""
        Sample Size Requirements for Covariance Estimation:
        
        Given:
        - n = {n_samples} samples
        - d = {n_dimensions} dimensions
        - Ratio: n/d = {ratio:.2f}
        
        Requirements:
        - Minimum: n > d (matrix must be full rank)
        - Recommended: n ‚â• 10d (stable estimation)
        - Ideal: n ‚â• 100d (asymptotic properties)
        
        Current status: {'PASS' if ratio >= 10 else 'WARNING' if ratio >= 1 else 'FAIL'}
        
        Mathematical Basis:
        - Covariance matrix has d(d+1)/2 unique parameters
        - Each sample provides d data points
        - Need n >> d to constrain all parameters
        """
        
        return VerificationFinding(
            claim=f"Sample size (n={n_samples}) adequate for d={n_dimensions} dimensions",
            status=status,
            confidence=confidence,
            priority=AuditPriority.CRITICAL,
            details=details,
            mathematical_proof=proof,
            recommendations=recommendations,
        )
    
    # =========================================================================
    # STATISTICAL VERIFICATION
    # =========================================================================
    
    def verify_cohens_d(
        self,
        mean1: float,
        mean2: float,
        std1: float,
        std2: float,
        n1: int,
        n2: int,
        claimed_d: Optional[float] = None,
    ) -> VerificationFinding:
        """
        Verify Cohen's d calculation is correct.
        
        Cohen's d = (M1 - M2) / SD_pooled
        """
        # Pooled standard deviation
        sd_pooled = math.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))
        
        # Cohen's d
        calculated_d = (mean1 - mean2) / sd_pooled
        
        # Magnitude interpretation
        abs_d = abs(calculated_d)
        if abs_d < 0.2:
            magnitude = "negligible"
        elif abs_d < 0.5:
            magnitude = "small"
        elif abs_d < 0.8:
            magnitude = "medium"
        elif abs_d < 1.0:
            magnitude = "large"
        elif abs_d < 2.0:
            magnitude = "very large"
        else:
            magnitude = "huge (requires extreme scrutiny)"
        
        # Verify against claimed value if provided
        if claimed_d is not None:
            discrepancy = abs(calculated_d - claimed_d)
            if discrepancy > 0.01:
                status = VerificationStatus.REJECTED
                confidence = 0.95
                details = f"CLAIM MISMATCH: Calculated d = {calculated_d:.3f}, claimed d = {claimed_d:.3f}"
            else:
                status = VerificationStatus.VALIDATED
                confidence = 0.95
                details = f"Verified: Cohen's d = {calculated_d:.3f} ({magnitude})"
        else:
            status = VerificationStatus.VALIDATED
            confidence = 0.90
            details = f"Calculated: Cohen's d = {calculated_d:.3f} ({magnitude})"
        
        # Special flag for extreme effect sizes
        if abs_d > 3.0:
            status = VerificationStatus.CONCERN
            details += "\n\nüö® EXTREME EFFECT SIZE: d > 3.0 requires exceptional scrutiny. "
            details += "Possible causes: measurement artifact, selection bias, or genuine massive effect."
        
        proof = f"""
        Cohen's d Calculation:
        
        Formula:
        d = (M‚ÇÅ - M‚ÇÇ) / SD_pooled
        
        Where:
        SD_pooled = ‚àö[((n‚ÇÅ-1)SD‚ÇÅ¬≤ + (n‚ÇÇ-1)SD‚ÇÇ¬≤) / (n‚ÇÅ+n‚ÇÇ-2)]
        
        Given:
        - M‚ÇÅ = {mean1}, SD‚ÇÅ = {std1}, n‚ÇÅ = {n1}
        - M‚ÇÇ = {mean2}, SD‚ÇÇ = {std2}, n‚ÇÇ = {n2}
        
        Calculation:
        SD_pooled = ‚àö[(({n1}-1)√ó{std1}¬≤ + ({n2}-1)√ó{std2}¬≤) / ({n1}+{n2}-2)]
                  = ‚àö[{((n1-1)*std1**2 + (n2-1)*std2**2):.3f} / {n1+n2-2}]
                  = {sd_pooled:.3f}
        
        d = ({mean1} - {mean2}) / {sd_pooled:.3f}
          = {calculated_d:.3f}
        
        Magnitude: {magnitude.upper()}
        """
        
        recommendations = []
        if abs_d > 2.0:
            recommendations.extend([
                "Extreme effect size requires verification: check for measurement artifacts",
                "Verify no selection bias in prompt categorization",
                "Replicate with independent sample",
            ])
        
        return VerificationFinding(
            claim=f"Cohen's d = {claimed_d if claimed_d else calculated_d:.3f}",
            status=status,
            confidence=confidence,
            priority=AuditPriority.CRITICAL if abs_d > 2.0 else AuditPriority.HIGH,
            details=details,
            mathematical_proof=proof,
            recommendations=recommendations,
        )
    
    def verify_p_value(
        self,
        test_statistic: float,
        df: int,
        claimed_p: Optional[float] = None,
        two_tailed: bool = True,
    ) -> VerificationFinding:
        """
        Verify p-value calculation from t-statistic.
        
        Uses scipy if available, otherwise provides formula.
        """
        try:
            from scipy import stats
            
            if two_tailed:
                calculated_p = 2 * (1 - stats.t.cdf(abs(test_statistic), df))
            else:
                calculated_p = 1 - stats.t.cdf(test_statistic, df)
            
            scipy_available = True
        except ImportError:
            calculated_p = None
            scipy_available = False
        
        # Verify against claimed
        if claimed_p is not None and calculated_p is not None:
            discrepancy = abs(calculated_p - claimed_p)
            if discrepancy > claimed_p * 0.1:  # 10% relative tolerance
                status = VerificationStatus.REJECTED
                details = f"P-value mismatch: calculated={calculated_p:.2e}, claimed={claimed_p:.2e}"
            else:
                status = VerificationStatus.VALIDATED
                details = f"P-value verified: {calculated_p:.2e}"
        else:
            status = VerificationStatus.VALIDATED if calculated_p else VerificationStatus.PENDING
            details = f"P-value calculation: t={test_statistic:.3f}, df={df}"
        
        # Extreme p-value warning
        if claimed_p is not None and claimed_p < 1e-20:
            status = VerificationStatus.CONCERN
            details += "\n\n‚ö†Ô∏è EXTREME P-VALUE: p < 10‚Åª¬≤‚Å∞ with moderate sample sizes is suspicious. "
            details += "Verify: (1) no pseudoreplication, (2) independence assumption holds, "
            details += "(3) test statistic calculation correct."
        
        proof = f"""
        P-value Calculation:
        
        Given:
        - Test statistic: t = {test_statistic:.4f}
        - Degrees of freedom: df = {df}
        - Two-tailed: {two_tailed}
        
        Formula:
        p = 2 √ó (1 - CDF_t(|t|, df))  [two-tailed]
        p = 1 - CDF_t(t, df)          [one-tailed]
        
        Where CDF_t is the cumulative distribution function of Student's t-distribution.
        """
        
        if calculated_p:
            proof += f"""
        Calculation:
        p = {calculated_p:.2e}
        """
        
        recommendations = []
        if claimed_p is not None and claimed_p < 1e-10:
            recommendations.extend([
                "Verify sample independence (no pseudoreplication)",
                "Check for multiple comparisons (needs Bonferroni/FDR correction)",
                "Consider practical significance, not just statistical",
            ])
        
        return VerificationFinding(
            claim=f"P-value = {claimed_p:.2e}" if claimed_p else "P-value calculation",
            status=status,
            confidence=0.90 if scipy_available else 0.70,
            priority=AuditPriority.HIGH,
            details=details,
            mathematical_proof=proof,
            recommendations=recommendations,
        )
    
    # =========================================================================
    # CAUSAL INFERENCE VERIFICATION
    # =========================================================================
    
    def verify_causal_claim(
        self,
        claim: str,
        evidence_type: str,  # "correlation", "activation_patching", "regression", "none"
        confounds_controlled: List[str],
        temporal_order: bool,
    ) -> VerificationFinding:
        """
        Verify a causal inference claim has adequate support.
        """
        if evidence_type == "activation_patching" and temporal_order:
            status = VerificationStatus.VALIDATED
            confidence = 0.85
            details = f"Causal claim '{claim}' supported by activation patching with correct temporal order."
        elif evidence_type == "correlation":
            status = VerificationStatus.CONCERN
            confidence = 0.60
            details = f"Causal claim '{claim}' based only on correlation. "
            details += "Correlation ‚â† causation. Needs activation patching or RCT."
        elif evidence_type == "none":
            status = VerificationStatus.REJECTED
            confidence = 0.95
            details = f"Causal claim '{claim}' has NO supporting evidence."
        else:
            status = VerificationStatus.CONCERN
            confidence = 0.70
            details = f"Causal claim '{claim}' has partial evidence. Needs stronger support."
        
        # Check confounds
        if confounds_controlled:
            details += f"\n\nConfounds controlled: {', '.join(confounds_controlled)}"
        else:
            details += "\n\n‚ö†Ô∏è No confounds explicitly controlled."
            if status == VerificationStatus.VALIDATED:
                status = VerificationStatus.CONCERN
        
        proof = """
        Causal Inference Requirements (Bradford Hill Criteria):
        
        1. Strength: Strong association (large effect size)
        2. Consistency: Replicated across studies/contexts
        3. Specificity: Cause leads to specific effect
        4. Temporality: Cause precedes effect
        5. Biological gradient: Dose-response relationship
        6. Plausibility: Mechanistically understandable
        7. Coherence: Fits with existing knowledge
        8. Experiment: Evidence from interventions
        9. Analogy: Similar to established causation
        
        Activation Patching Requirements:
        - Clean causal path (no confounding)
        - Temporal ordering verified
        - Dose-response demonstrated
        - Specificity (intervention affects target only)
        """
        
        recommendations = []
        if evidence_type == "correlation":
            recommendations.extend([
                "Conduct activation patching experiment",
                "Control for confounds (prompt length, complexity)",
                "Establish temporal ordering",
            ])
        
        if not confounds_controlled:
            recommendations.extend([
                "Identify potential confounding variables",
                "Use propensity score matching or stratification",
                "Add regression controls",
            ])
        
        return VerificationFinding(
            claim=f"Causal: {claim}",
            status=status,
            confidence=confidence,
            priority=AuditPriority.HIGH,
            details=details,
            mathematical_proof=proof,
            recommendations=recommendations,
        )
    
    # =========================================================================
    # TRANSFORMER CIRCUIT VERIFICATION
    # =========================================================================
    
    def verify_attention_mechanism(self) -> VerificationFinding:
        """Verify attention mechanism mathematical formulation."""
        proof = """
        Multi-Head Attention Verification:
        
        Standard Formulation:
        Attention(Q, K, V) = softmax(QK·µÄ/‚àöd‚Çñ)V
        
        Where:
        - Q = XW_Q (queries) ‚àà ‚Ñù^(n√ód‚Çñ)
        - K = XW_K (keys) ‚àà ‚Ñù^(n√ód‚Çñ)
        - V = XW_V (values) ‚àà ‚Ñù^(n√ód·µ•)
        - d‚Çñ = dimension of key vectors
        - n = sequence length
        
        Checks:
        1. Scaling factor ‚àöd‚Çñ prevents softmax saturation
           - Without scaling: QK·µÄ values large ‚Üí softmax ‚Üí one-hot
           - With scaling: variance controlled
        
        2. Softmax applied row-wise:
           attention_weights[i,j] = exp(Q[i]¬∑K[j]/‚àöd‚Çñ) / Œ£‚Çñ exp(Q[i]¬∑K[k]/‚àöd‚Çñ)
        
        3. Attention weights sum to 1 per position:
           Œ£‚±º attention_weights[i,j] = 1 for all i
        
        4. Output dimension matches value dimension:
           output ‚àà ‚Ñù^(n√ód·µ•)
        
        Multi-Head Extension:
        MultiHead(Q,K,V) = Concat(head‚ÇÅ,...,head‚Çï)W_O
        where head·µ¢ = Attention(QW_Q‚Å±, KW_K‚Å±, VW_V‚Å±)
        """
        
        return VerificationFinding(
            claim="Attention mechanism formulation is mathematically correct",
            status=VerificationStatus.VALIDATED,
            confidence=0.98,
            priority=AuditPriority.HIGH,
            details="Standard attention formulation verified. Scaling factor ‚àöd‚Çñ is critical for numerical stability.",
            mathematical_proof=proof,
            recommendations=[
                "Verify implementation uses correct scaling",
                "Check attention weights sum to 1.0 (numerical precision)",
            ],
        )
    
    def verify_svd_stability(self, use_double_precision: bool = True) -> VerificationFinding:
        """Verify SVD computation is numerically stable."""
        if use_double_precision:
            status = VerificationStatus.VALIDATED
            confidence = 0.95
            details = "Using float64 (double precision) for SVD. This is REQUIRED for stability with high-dimensional data."
            recommendations = []
        else:
            status = VerificationStatus.CONCERN
            confidence = 0.70
            details = "Using float32 (single precision) for SVD. Risk of numerical instability with high-dimensional data."
            recommendations = [
                "Switch to float64 for SVD operations",
                "Add numerical stability checks for singular values",
            ]
        
        proof = """
        SVD Numerical Stability:
        
        SVD: M = UŒ£V·µÄ
        
        Precision Requirements:
        - Float32 (single): ~7 decimal digits
        - Float64 (double): ~16 decimal digits
        
        For d=4096 dimensional data:
        - Condition number can be 10‚Å∂ or higher
        - Float32 loses precision: 7 - 6 = 1 digit remaining
        - Float64 maintains precision: 16 - 6 = 10 digits
        
        Recommendation:
        ALWAYS use float64 for SVD in high-dimensional settings.
        """
        
        return VerificationFinding(
            claim="SVD uses numerically stable precision",
            status=status,
            confidence=confidence,
            priority=AuditPriority.CRITICAL,
            details=details,
            mathematical_proof=proof,
            recommendations=recommendations,
        )
    
    # =========================================================================
    # COMPREHENSIVE AUDITS
    # =========================================================================
    
    def audit_rv_claims(
        self,
        repo_path: Optional[str] = None,
        confidence_threshold: float = 0.90,
    ) -> FormalVerificationReport:
        """
        Comprehensive audit of all R_V-related mathematical claims.
        
        This is the primary entry point for AIKAGRYA verification.
        """
        report = FormalVerificationReport(
            title="AIKAGRYA R_V Mathematical Verification Report",
            summary="Formal verification of R_V metric mathematical soundness, statistical methods, and causal claims.",
            overall_status=VerificationStatus.PENDING,
            overall_confidence=0.0,
        )
        
        # 1. R_V Definition
        report.add_finding(self.verify_rv_definition())
        
        # 2. Covariance Computation
        report.add_finding(self.verify_covariance_computation())
        
        # 3. Sample Size (example values - would be parameterized in real use)
        report.add_finding(self.verify_sample_size_requirements(
            n_samples=1000,
            n_dimensions=64,
        ))
        
        # 4. Cohen's d (example: claimed d = -5.57)
        finding = self.verify_cohens_d(
            mean1=0.65,  # Recursive R_V
            mean2=1.02,  # Baseline R_V
            std1=0.08,
            std2=0.12,
            n1=100,
            n2=100,
            claimed_d=-5.57,
        )
        report.add_finding(finding)
        
        # 5. P-value
        report.add_finding(self.verify_p_value(
            test_statistic=-39.5,  # Would give p < 10^-30
            df=99,
            claimed_p=1e-30,
        ))
        
        # 6. Causal Claims
        report.add_finding(self.verify_causal_claim(
            claim="R_V contraction causes L4 phenomenology",
            evidence_type="correlation",  # Would be "activation_patching" if validated
            confounds_controlled=["prompt length"],
            temporal_order=True,
        ))
        
        # 7. Transformer Circuits
        report.add_finding(self.verify_attention_mechanism())
        report.add_finding(self.verify_svd_stability(use_double_precision=True))
        
        # Calculate overall status
        if report.critical_issues > 0:
            report.overall_status = VerificationStatus.REJECTED
        elif report.warnings > 0:
            report.overall_status = VerificationStatus.CONCERN
        else:
            report.overall_status = VerificationStatus.VALIDATED
        
        # Calculate overall confidence
        if report.findings:
            report.overall_confidence = sum(f.confidence for f in report.findings) / len(report.findings)
        
        # Update summary
        report.summary = (
            f"Mathematical verification complete. "
            f"{report.validated_claims} claims validated, "
            f"{report.warnings} warnings, "
            f"{report.critical_issues} critical issues. "
            f"Overall confidence: {report.overall_confidence:.1%}. "
            f"Status: {report.overall_status.value.upper()}."
        )
        
        return report
    
    def audit_file(self, file_path: Union[str, Path]) -> FormalVerificationReport:
        """
        Audit a specific Python file for mathematical correctness.
        
        Performs static analysis to identify:
        - Covariance computations
        - Statistical tests
        - Matrix operations
        - Potential numerical issues
        """
        file_path = Path(file_path)
        
        report = FormalVerificationReport(
            title=f"File Audit: {file_path.name}",
            summary=f"Static analysis of {file_path}",
            overall_status=VerificationStatus.PENDING,
            overall_confidence=0.0,
        )
        
        try:
            with open(file_path, 'r') as f:
                code = f.read()
            
            # Parse AST
            tree = ast.parse(code)
            
            # Check for covariance computation
            if 'cov' in code or 'covariance' in code.lower():
                report.add_finding(VerificationFinding(
                    claim="Covariance computation detected",
                    status=VerificationStatus.CONCERN,
                    confidence=0.70,
                    priority=AuditPriority.HIGH,
                    details="Verify uses (n-1) denominator for unbiased estimator",
                    code_reference=f"{file_path}: covariance computation",
                ))
            
            # Check for SVD
            if 'svd' in code.lower() or 'linalg.svd' in code:
                has_float64 = 'float64' in code or 'double' in code.lower()
                report.add_finding(VerificationFinding(
                    claim="SVD computation detected",
                    status=VerificationStatus.VALIDATED if has_float64 else VerificationStatus.CONCERN,
                    confidence=0.80 if has_float64 else 0.60,
                    priority=AuditPriority.HIGH,
                    details="SVD found. " + ("Float64 detected ‚úì" if has_float64 else "VERIFY: Uses float64 for stability"),
                    code_reference=f"{file_path}: SVD operation",
                    recommendations=[] if has_float64 else ["Add .double() or .to(torch.float64) before SVD"],
                ))
            
            # Check for determinants
            if 'det' in code or 'determinant' in code.lower():
                has_logdet = 'logdet' in code or 'slogdet' in code
                report.add_finding(VerificationFinding(
                    claim="Matrix determinant computation detected",
                    status=VerificationStatus.VALIDATED if has_logdet else VerificationStatus.CONCERN,
                    confidence=0.85 if has_logdet else 0.65,
                    priority=AuditPriority.MEDIUM,
                    details="Determinant found. " + ("Log-determinant detected ‚úì" if has_logdet else "Consider logdet for numerical stability"),
                    recommendations=[] if has_logdet else ["Use torch.logdet() or np.linalg.slogdet()"],
                ))
            
        except Exception as e:
            report.add_finding(VerificationFinding(
                claim=f"Failed to audit {file_path}",
                status=VerificationStatus.REJECTED,
                confidence=1.0,
                priority=AuditPriority.CRITICAL,
                details=f"Error: {str(e)}",
            ))
        
        # Update overall status
        if report.critical_issues > 0:
            report.overall_status = VerificationStatus.REJECTED
        elif report.warnings > 0 or any(f.status == VerificationStatus.CONCERN for f in report.findings):
            report.overall_status = VerificationStatus.CONCERN
        else:
            report.overall_status = VerificationStatus.VALIDATED
        
        return report
    
    def save_report(
        self,
        report: FormalVerificationReport,
        output_path: Union[str, Path],
        format: str = "markdown",
    ) -> Path:
        """
        Save verification report to file.
        
        Args:
            report: The verification report to save
            output_path: Where to save the report
            format: 'markdown', 'json', or 'both'
        
        Returns:
            Path to saved file
        """
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        if format in ["markdown", "both"]:
            md_path = output_path.with_suffix('.md')
            with open(md_path, 'w') as f:
                f.write(report.to_markdown())
            self._log(f"Saved markdown report to {md_path}")
        
        if format in ["json", "both"]:
            json_path = output_path.with_suffix('.json')
            with open(json_path, 'w') as f:
                json.dump(report.to_dict(), f, indent=2)
            self._log(f"Saved JSON report to {json_path}")
        
        return output_path


# =============================================================================
# COMMAND-LINE INTERFACE
# =============================================================================

def main():
    """CLI entry point for mathematical verification."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Mathematical Verification Agent for AIKAGRYA"
    )
    parser.add_argument(
        "--audit-rv",
        action="store_true",
        help="Run comprehensive R_V audit",
    )
    parser.add_argument(
        "--file",
        type=str,
        help="Audit specific file",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="~/clawd/reports/math_verification_report",
        help="Output path for report",
    )
    parser.add_argument(
        "--format",
        choices=["markdown", "json", "both"],
        default="both",
        help="Report format",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging",
    )
    
    args = parser.parse_args()
    
    # Initialize auditor
    auditor = MathAuditor(debug=args.debug)
    
    # Run requested audit
    if args.audit_rv:
        print("üîç Running comprehensive R_V mathematical verification...")
        report = auditor.audit_rv_claims()
    elif args.file:
        print(f"üîç Auditing file: {args.file}")
        report = auditor.audit_file(args.file)
    else:
        print("üîç Running default R_V audit (use --audit-rv or --file)...")
        report = auditor.audit_rv_claims()
    
    # Save report
    output_path = Path(args.output).expanduser()
    auditor.save_report(report, output_path, format=args.format)
    
    # Print summary
    print("\n" + "="*60)
    print(report.summary)
    print("="*60)
    
    # Exit code based on status
    if report.overall_status == VerificationStatus.REJECTED:
        print("\nüö® CRITICAL ISSUES FOUND. Review required.")
        return 1
    elif report.overall_status == VerificationStatus.CONCERN:
        print("\n‚ö†Ô∏è  WARNINGS FOUND. Review recommended.")
        return 2
    else:
        print("\n‚úÖ All checks passed.")
        return 0


if __name__ == "__main__":
    exit(main())
