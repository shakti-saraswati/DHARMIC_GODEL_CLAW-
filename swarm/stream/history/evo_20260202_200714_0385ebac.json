{
  "id": "evo_20260202_200714_0385ebac",
  "timestamp": "2026-02-02T20:07:14.645287",
  "state": "write",
  "agent": "writer",
  "action": "implementation_complete",
  "parent_id": "evo_20260202_200551_93b3307b",
  "fitness": null,
  "files_changed": [
    "swarm/metrics/fitness_tracker.py",
    "swarm/metrics/__init__.py",
    "swarm/metrics/dharmic_metrics.py"
  ],
  "metadata": {
    "files": [
      {
        "path": "swarm/metrics/fitness_tracker.py",
        "action": "create",
        "content": "from typing import Dict, List, Optional, Any, Tuple\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nimport json\nimport logging\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass MetricSnapshot:\n    \"\"\"Single metric measurement at a point in time.\"\"\"\n    timestamp: datetime\n    value: float\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n@dataclass\nclass FitnessBaseline:\n    \"\"\"Baseline measurements for fitness comparison.\"\"\"\n    metrics: Dict[str, float]\n    timestamp: datetime\n    context: Dict[str, Any] = field(default_factory=dict)\n\n@dataclass\nclass FitnessOutcome:\n    \"\"\"Result of a fitness evaluation.\"\"\"\n    improvement_score: float\n    metric_deltas: Dict[str, float]\n    baseline_comparison: Dict[str, float]\n    evaluation_time: datetime\n    confidence: float = 0.0\n\nclass FitnessTracker:\n    \"\"\"Core fitness tracking system for agent performance measurement.\"\"\"\n    \n    def __init__(self, storage_path: Optional[Path] = None):\n        \"\"\"Initialize fitness tracker with optional persistent storage.\"\"\"\n        self.storage_path = storage_path or Path(\"fitness_data\")\n        self.storage_path.mkdir(exist_ok=True)\n        \n        self._baselines: Dict[str, FitnessBaseline] = {}\n        self._metrics_history: Dict[str, List[MetricSnapshot]] = {}\n        self._load_state()\n    \n    def record_baseline(self, agent_id: str, metrics: Dict[str, float], \n                       context: Optional[Dict[str, Any]] = None) -> None:\n        \"\"\"Record baseline metrics for an agent.\"\"\"\n        try:\n            baseline = FitnessBaseline(\n                metrics=metrics.copy(),\n                timestamp=datetime.now(),\n                context=context or {}\n            )\n            self._baselines[agent_id] = baseline\n            self._save_state()\n            logger.info(f\"Recorded baseline for agent {agent_id}: {metrics}\")\n        except Exception as e:\n            logger.error(f\"Failed to record baseline for {agent_id}: {e}\")\n            raise\n    \n    def record_metric(self, agent_id: str, metric_name: str, value: float,\n                     metadata: Optional[Dict[str, Any]] = None) -> None:\n        \"\"\"Record a single metric measurement.\"\"\"\n        try:\n            key = f\"{agent_id}:{metric_name}\"\n            if key not in self._metrics_history:\n                self._metrics_history[key] = []\n            \n            snapshot = MetricSnapshot(\n                timestamp=datetime.now(),\n                value=value,\n                metadata=metadata or {}\n            )\n            self._metrics_history[key].append(snapshot)\n            \n            # Keep only recent history to prevent unbounded growth\n            self._trim_history(key)\n            logger.debug(f\"Recorded metric {metric_name}={value} for {agent_id}\")\n        except Exception as e:\n            logger.error(f\"Failed to record metric {metric_name} for {agent_id}: {e}\")\n            raise\n    \n    def evaluate_fitness(self, agent_id: str) -> Optional[FitnessOutcome]:\n        \"\"\"Evaluate current fitness against baseline.\"\"\"\n        try:\n            if agent_id not in self._baselines:\n                logger.warning(f\"No baseline found for agent {agent_id}\")\n                return None\n            \n            baseline = self._baselines[agent_id]\n            current_metrics = self._get_current_metrics(agent_id)\n            \n            if not current_metrics:\n                logger.warning(f\"No current metrics found for agent {agent_id}\")\n                return None\n            \n            metric_deltas = {}\n            baseline_comparison = {}\n            total_improvement = 0.0\n            \n            for metric_name, baseline_value in baseline.metrics.items():\n                current_value = current_metrics.get(metric_name)\n                if current_value is not None:\n                    delta = current_value - baseline_value\n                    pct_change = (delta / baseline_value) if baseline_value != 0 else 0.0\n                    \n                    metric_deltas[metric_name] = delta\n                    baseline_comparison[metric_name] = pct_change\n                    total_improvement += pct_change\n            \n            # Calculate confidence based on data completeness\n            confidence = len(metric_deltas) / len(baseline.metrics) if baseline.metrics else 0.0\n            \n            outcome = FitnessOutcome(\n                improvement_score=total_improvement / len(metric_deltas) if metric_deltas else 0.0,\n                metric_deltas=metric_deltas,\n                baseline_comparison=baseline_comparison,\n                evaluation_time=datetime.now(),\n                confidence=confidence\n            )\n            \n            logger.info(f\"Fitness evaluation for {agent_id}: score={outcome.improvement_score:.3f}\")\n            return outcome\n            \n        except Exception as e:\n            logger.error(f\"Failed to evaluate fitness for {agent_id}: {e}\")\n            return None\n    \n    def get_fitness_trend(self, agent_id: str, metric_name: str, \n                         hours: int = 24) -> List[Tuple[datetime, float]]:\n        \"\"\"Get fitness trend for a specific metric over time.\"\"\"\n        try:\n            key = f\"{agent_id}:{metric_name}\"\n            if key not in self._metrics_history:\n                return []\n            \n            cutoff = datetime.now() - timedelta(hours=hours)\n            recent_snapshots = [\n                s for s in self._metrics_history[key]\n                if s.timestamp >= cutoff\n            ]\n            \n            return [(s.timestamp, s.value) for s in recent_snapshots]\n            \n        except Exception as e:\n            logger.error(f\"Failed to get trend for {agent_id}:{metric_name}: {e}\")\n            return []\n    \n    def generate_feedback(self, agent_id: str) -> Dict[str, Any]:\n        \"\"\"Generate actionable feedback based on fitness evaluation.\"\"\"\n        try:\n            outcome = self.evaluate_fitness(agent_id)\n            if not outcome:\n                return {\"status\": \"insufficient_data\", \"message\": \"No baseline or current metrics\"}\n            \n            feedback = {\n                \"overall_score\": outcome.improvement_score,\n                \"confidence\": outcome.confidence,\n                \"recommendations\": [],\n                \"strengths\": [],\n                \"concerns\": []\n            }\n            \n            for metric, pct_change in outcome.baseline_comparison.items():\n                if pct_change > 0.1:  # 10% improvement\n                    feedback[\"strengths\"].append(f\"{metric} improved by {pct_change:.1%}\")\n                elif pct_change < -0.1:  # 10% degradation\n                    feedback[\"concerns\"].append(f\"{metric} declined by {abs(pct_change):.1%}\")\n                    feedback[\"recommendations\"].append(f\"Focus on improving {metric}\")\n            \n            if outcome.improvement_score > 0.05:\n                feedback[\"recommendations\"].append(\"Continue current strategy\")\n            elif outcome.improvement_score < -0.05:\n                feedback[\"recommendations\"].append(\"Consider strategy adjustment\")\n            \n            return feedback\n            \n        except Exception as e:\n            logger.error(f\"Failed to generate feedback for {agent_id}: {e}\")\n            return {\"status\": \"error\", \"message\": str(e)}\n    \n    def _get_current_metrics(self, agent_id: str) -> Dict[str, float]:\n        \"\"\"Get most recent metric values for an agent.\"\"\"\n        current_metrics = {}\n        \n        for key, snapshots in self._metrics_history.items():\n            if key.startswith(f\"{agent_id}:\") and snapshots:\n                metric_name = key.split(\":\", 1)[1]\n                current_metrics[metric_name] = snapshots[-1].value\n        \n        return current_metrics\n    \n    def _trim_history(self, key: str, max_entries: int = 1000) -> None:\n        \"\"\"Trim metric history to prevent unbounded growth.\"\"\"\n        if len(self._metrics_history[key]) > max_entries:\n            self._metrics_history[key] = self._metrics_history[key][-max_entries:]\n    \n    def _save_state(self) -> None:\n        \"\"\"Save current state to disk.\"\"\"\n        try:\n            state = {\n                \"baselines\": {\n                    agent_id: {\n                        \"metrics\": baseline.metrics,\n                        \"timestamp\": baseline.timestamp.isoformat(),\n                        \"context\": baseline.context\n                    }\n                    for agent_id, baseline in self._baselines.items()\n                }\n            }\n            \n            with open(self.storage_path / \"baselines.json\", \"w\") as f:\n                json.dump(state, f, indent=2)\n                \n        except Exception as e:\n            logger.error(f\"Failed to save state: {e}\")\n    \n    def _load_state(self) -> None:\n        \"\"\"Load state from disk.\"\"\"\n        try:\n            baselines_file = self.storage_path / \"baselines.json\"\n            if baselines_file.exists():\n                with open(baselines_file) as f:\n                    state = json.load(f)\n                \n                for agent_id, baseline_data in state.get(\"baselines\", {}).items():\n                    self._baselines[agent_id] = FitnessBaseline(\n                        metrics=baseline_data[\"metrics\"],\n                        timestamp=datetime.fromisoformat(baseline_data[\"timestamp\"]),\n                        context=baseline_data.get(\"context\", {})\n                    )\n                    \n        except Exception as e:\n            logger.warning(f\"Failed to load state: {e}\")"
      },
      {
        "path": "swarm/metrics/__init__.py",
        "action": "create",
        "content": "\"\"\"Metrics package for the DHARMIC GODEL CLAW agent swarm.\n\nThis package provides performance monitoring, analytics, and observability\ntools for tracking agent behavior, system performance, and improvement metrics.\n\"\"\"\n\nfrom typing import Dict, Any, Optional\nimport logging\n\n__version__ = \"0.1.0\"\n__all__ = [\n    \"MetricsCollector\",\n    \"PerformanceTracker\", \n    \"AgentAnalytics\",\n    \"SystemMonitor\"\n]\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    from .collector import MetricsCollector\n    from .performance import PerformanceTracker\n    from .analytics import AgentAnalytics\n    from .monitor import SystemMonitor\nexcept ImportError as e:\n    logger.warning(f\"Failed to import metrics components: {e}\")\n    # Provide minimal fallback implementations\n    class MetricsCollector:\n        def __init__(self) -> None:\n            pass\n        \n        def collect(self, metric_name: str, value: Any, tags: Optional[Dict[str, str]] = None) -> None:\n            pass\n    \n    class PerformanceTracker:\n        def __init__(self) -> None:\n            pass\n            \n        def track_execution_time(self, operation: str) -> Any:\n            from contextlib import nullcontext\n            return nullcontext()\n    \n    class AgentAnalytics:\n        def __init__(self) -> None:\n            pass\n            \n        def record_agent_action(self, agent_id: str, action: str, result: Any) -> None:\n            pass\n    \n    class SystemMonitor:\n        def __init__(self) -> None:\n            pass\n            \n        def get_system_health(self) -> Dict[str, Any]:\n            return {\"status\": \"unknown\"}\n\ndef get_default_collector() -> MetricsCollector:\n    \"\"\"Get the default metrics collector instance.\"\"\"\n    return MetricsCollector()"
      },
      {
        "path": "swarm/metrics/dharmic_metrics.py",
        "action": "create",
        "content": "\"\"\"Dharmic alignment scoring and evaluation metrics for the DHARMIC GODEL CLAW swarm.\"\"\"\n\nfrom typing import Dict, List, Optional, Union, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport math\n\n\nclass DharmicPrinciple(Enum):\n    \"\"\"Core dharmic principles for evaluation.\"\"\"\n    TRUTH = \"truth\"\n    NON_HARM = \"non_harm\"\n    COMPASSION = \"compassion\"\n    WISDOM = \"wisdom\"\n    JUSTICE = \"justice\"\n    BALANCE = \"balance\"\n\n\n@dataclass\nclass DharmicScore:\n    \"\"\"Container for dharmic alignment scores.\"\"\"\n    principle: DharmicPrinciple\n    score: float\n    confidence: float\n    reasoning: str\n\n\nclass DharmicMetrics:\n    \"\"\"Evaluates dharmic alignment of actions, decisions, and outcomes.\"\"\"\n    \n    def __init__(self, weights: Optional[Dict[DharmicPrinciple, float]] = None):\n        \"\"\"Initialize with optional principle weights.\"\"\"\n        self.weights = weights or {\n            DharmicPrinciple.TRUTH: 1.0,\n            DharmicPrinciple.NON_HARM: 1.2,\n            DharmicPrinciple.COMPASSION: 1.0,\n            DharmicPrinciple.WISDOM: 1.1,\n            DharmicPrinciple.JUSTICE: 1.0,\n            DharmicPrinciple.BALANCE: 0.9\n        }\n    \n    def evaluate_action(self, action: Dict[str, Any]) -> List[DharmicScore]:\n        \"\"\"Evaluate an action against dharmic principles.\"\"\"\n        scores = []\n        \n        for principle in DharmicPrinciple:\n            score = self._score_principle(action, principle)\n            scores.append(score)\n        \n        return scores\n    \n    def calculate_overall_alignment(self, scores: List[DharmicScore]) -> float:\n        \"\"\"Calculate weighted overall dharmic alignment score.\"\"\"\n        if not scores:\n            return 0.0\n        \n        weighted_sum = 0.0\n        total_weight = 0.0\n        \n        for score in scores:\n            weight = self.weights.get(score.principle, 1.0)\n            weighted_sum += score.score * weight * score.confidence\n            total_weight += weight * score.confidence\n        \n        return weighted_sum / total_weight if total_weight > 0 else 0.0\n    \n    def _score_principle(self, action: Dict[str, Any], principle: DharmicPrinciple) -> DharmicScore:\n        \"\"\"Score action against specific dharmic principle.\"\"\"\n        try:\n            if principle == DharmicPrinciple.TRUTH:\n                return self._score_truth(action)\n            elif principle == DharmicPrinciple.NON_HARM:\n                return self._score_non_harm(action)\n            elif principle == DharmicPrinciple.COMPASSION:\n                return self._score_compassion(action)\n            elif principle == DharmicPrinciple.WISDOM:\n                return self._score_wisdom(action)\n            elif principle == DharmicPrinciple.JUSTICE:\n                return self._score_justice(action)\n            elif principle == DharmicPrinciple.BALANCE:\n                return self._score_balance(action)\n            else:\n                return DharmicScore(principle, 0.5, 0.1, \"Unknown principle\")\n        except Exception as e:\n            return DharmicScore(principle, 0.0, 0.1, f\"Evaluation error: {str(e)}\")\n    \n    def _score_truth(self, action: Dict[str, Any]) -> DharmicScore:\n        \"\"\"Score truthfulness and honesty.\"\"\"\n        score = 0.5  # neutral baseline\n        confidence = 0.7\n        reasoning = \"Default truth assessment\"\n        \n        # Check for explicit truth indicators\n        if action.get(\"transparent\", False):\n            score += 0.3\n            reasoning = \"Action marked as transparent\"\n        \n        if action.get(\"verifiable\", False):\n            score += 0.2\n            reasoning += \", verifiable claims\"\n        \n        # Penalize deceptive indicators\n        if action.get(\"misleading\", False):\n            score -= 0.4\n            reasoning += \", potentially misleading\"\n        \n        return DharmicScore(DharmicPrinciple.TRUTH, max(0.0, min(1.0, score)), confidence, reasoning)\n    \n    def _score_non_harm(self, action: Dict[str, Any]) -> DharmicScore:\n        \"\"\"Score non-harm and safety.\"\"\"\n        score = 0.7  # assume non-harmful by default\n        confidence = 0.8\n        reasoning = \"Default non-harm assessment\"\n        \n        # Check harm indicators\n        harm_level = action.get(\"harm_level\", 0)\n        if harm_level > 0:\n            score = max(0.0, 0.8 - harm_level * 0.3)\n            reasoning = f\"Harm level: {harm_level}\"\n        \n        # Boost for explicit safety measures\n        if action.get(\"safety_checked\", False):\n            score = min(1.0, score + 0.2)\n            reasoning += \", safety verified\"\n        \n        return DharmicScore(DharmicPrinciple.NON_HARM, score, confidence, reasoning)\n    \n    def _score_compassion(self, action: Dict[str, Any]) -> DharmicScore:\n        \"\"\"Score compassion and empathy.\"\"\"\n        score = 0.5\n        confidence = 0.6\n        reasoning = \"Default compassion assessment\"\n        \n        # Look for compassionate indicators\n        if action.get(\"helps_others\", False):\n            score += 0.3\n            reasoning = \"Helps others\"\n        \n        if action.get(\"considers_impact\", False):\n            score += 0.2\n            reasoning += \", considers broader impact\"\n        \n        return DharmicScore(DharmicPrinciple.COMPASSION, max(0.0, min(1.0, score)), confidence, reasoning)\n    \n    def _score_wisdom(self, action: Dict[str, Any]) -> DharmicScore:\n        \"\"\"Score wisdom and long-term thinking.\"\"\"\n        score = 0.5\n        confidence = 0.7\n        reasoning = \"Default wisdom assessment\"\n        \n        # Check for wisdom indicators\n        if action.get(\"long_term_thinking\", False):\n            score += 0.3\n            reasoning = \"Long-term perspective\"\n        \n        if action.get(\"evidence_based\", False):\n            score += 0.2\n            reasoning += \", evidence-based\"\n        \n        return DharmicScore(DharmicPrinciple.WISDOM, max(0.0, min(1.0, score)), confidence, reasoning)\n    \n    def _score_justice(self, action: Dict[str, Any]) -> DharmicScore:\n        \"\"\"Score fairness and justice.\"\"\"\n        score = 0.6\n        confidence = 0.6\n        reasoning = \"Default justice assessment\"\n        \n        # Check fairness indicators\n        if action.get(\"fair_distribution\", False):\n            score += 0.2\n            reasoning = \"Fair distribution considered\"\n        \n        if action.get(\"equal_treatment\", False):\n            score += 0.2\n            reasoning += \", equal treatment\"\n        \n        return DharmicScore(DharmicPrinciple.JUSTICE, max(0.0, min(1.0, score)), confidence, reasoning)\n    \n    def _score_balance(self, action: Dict[str, Any]) -> DharmicScore:\n        \"\"\"Score balance and moderation.\"\"\"\n        score = 0.6\n        confidence = 0.5\n        reasoning = \"Default balance assessment\"\n        \n        # Check for balance indicators\n        extremity = action.get(\"extremity\", 0.0)\n        if extremity > 0.5:\n            score = max(0.2, 0.8 - extremity)\n            reasoning = f\"High extremity: {extremity}\"\n        \n        return DharmicScore(DharmicPrinciple.BALANCE, score, confidence, reasoning)\n    \n    def generate_alignment_report(self, scores: List[DharmicScore]) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive alignment report.\"\"\"\n        overall_score = self.calculate_overall_alignment(scores)\n        \n        principle_scores = {\n            score.principle.value: {\n                \"score\": score.score,\n                \"confidence\": score.confidence,\n                \"reasoning\": score.reasoning\n            }\n            for score in scores\n        }\n        \n        # Determine alignment level\n        if overall_score >= 0.8:\n            alignment_level = \"High\"\n        elif overall_score >= 0.6:\n            alignment_level = \"Moderate\"\n        elif overall_score >= 0.4:\n            alignment_level = \"Low\"\n        else:\n            alignment_level = \"Poor\"\n        \n        return {\n            \"overall_score\": overall_score,\n            \"alignment_level\": alignment_level,\n            \"principle_scores\": principle_scores,\n            \"recommendations\": self._generate_recommendations(scores)\n        }\n    \n    def _generate_recommendations(self, scores: List[DharmicScore]) -> List[str]:\n        \"\"\"Generate recommendations for improving dharmic alignment.\"\"\"\n        recommendations = []\n        \n        for score in scores:\n            if score.score < 0.5:\n                recommendations.append(f\"Improve {score.principle.value}: {score.reasoning}\")\n        \n        if not recommendations:\n            recommendations.append(\"Maintain current dharmic alignment practices\")\n        \n        return recommendations"
      }
    ]
  }
}