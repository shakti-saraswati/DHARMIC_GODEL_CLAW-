{
  "id": "evo_20260203_080937_f946d924",
  "timestamp": "2026-02-03T08:09:37.019281",
  "state": "write",
  "agent": "writer",
  "action": "implementation_complete",
  "parent_id": "evo_20260203_080815_8ea4cb39",
  "fitness": null,
  "files_changed": [
    "swarm/evaluation.py",
    "swarm/models.py",
    "swarm/orchestrator.py"
  ],
  "metadata": {
    "files": [
      {
        "path": "swarm/evaluation.py",
        "action": "create",
        "content": "\"\"\"Core evaluation system for the DHARMIC GODEL CLAW swarm.\"\"\"\n\nimport asyncio\nimport logging\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nfrom .types import Proposal, EvaluationResult, Agent\n\n\nclass EvaluationStatus(Enum):\n    \"\"\"Status of an evaluation.\"\"\"\n    PENDING = \"pending\"\n    IN_PROGRESS = \"in_progress\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\n\n@dataclass\nclass EvaluationContext:\n    \"\"\"Context for evaluation including metrics and constraints.\"\"\"\n    proposal: Proposal\n    current_metrics: Dict[str, float]\n    resource_limits: Dict[str, Any]\n    safety_constraints: List[str]\n    timestamp: datetime\n\n\nclass ProposalEvaluator:\n    \"\"\"Evaluates proposals for safety, feasibility, and potential impact.\"\"\"\n    \n    def __init__(self, safety_threshold: float = 0.7, impact_threshold: float = 0.5):\n        \"\"\"Initialize evaluator with thresholds.\"\"\"\n        self.safety_threshold = safety_threshold\n        self.impact_threshold = impact_threshold\n        self.logger = logging.getLogger(__name__)\n        self._evaluation_cache: Dict[str, EvaluationResult] = {}\n    \n    async def evaluate_proposal(self, proposal: Proposal, context: EvaluationContext) -> EvaluationResult:\n        \"\"\"Evaluate a single proposal comprehensively.\"\"\"\n        try:\n            # Check cache first\n            cache_key = self._get_cache_key(proposal, context)\n            if cache_key in self._evaluation_cache:\n                return self._evaluation_cache[cache_key]\n            \n            # Run parallel evaluations\n            safety_score, impact_score, feasibility_score = await asyncio.gather(\n                self._evaluate_safety(proposal, context),\n                self._evaluate_impact(proposal, context),\n                self._evaluate_feasibility(proposal, context)\n            )\n            \n            # Calculate overall score\n            overall_score = self._calculate_overall_score(\n                safety_score, impact_score, feasibility_score\n            )\n            \n            # Make decision\n            approved = (\n                safety_score >= self.safety_threshold and\n                impact_score >= self.impact_threshold and\n                feasibility_score >= 0.6 and\n                overall_score >= 0.6\n            )\n            \n            result = EvaluationResult(\n                proposal_id=proposal.id,\n                approved=approved,\n                safety_score=safety_score,\n                impact_score=impact_score,\n                feasibility_score=feasibility_score,\n                overall_score=overall_score,\n                reasoning=self._generate_reasoning(\n                    safety_score, impact_score, feasibility_score, approved\n                ),\n                timestamp=datetime.now()\n            )\n            \n            # Cache result\n            self._evaluation_cache[cache_key] = result\n            return result\n            \n        except Exception as e:\n            self.logger.error(f\"Evaluation failed for proposal {proposal.id}: {e}\")\n            return EvaluationResult(\n                proposal_id=proposal.id,\n                approved=False,\n                safety_score=0.0,\n                impact_score=0.0,\n                feasibility_score=0.0,\n                overall_score=0.0,\n                reasoning=f\"Evaluation failed: {str(e)}\",\n                timestamp=datetime.now()\n            )\n    \n    async def _evaluate_safety(self, proposal: Proposal, context: EvaluationContext) -> float:\n        \"\"\"Evaluate safety of proposal.\"\"\"\n        score = 1.0\n        \n        # Check against safety constraints\n        for constraint in context.safety_constraints:\n            if self._violates_constraint(proposal, constraint):\n                score *= 0.5\n        \n        # Penalize high-risk operations\n        if any(keyword in proposal.description.lower() for keyword in \n               ['delete', 'remove', 'destroy', 'shutdown']):\n            score *= 0.7\n        \n        # Reward gradual changes\n        if 'incremental' in proposal.description.lower():\n            score *= 1.1\n        \n        return min(score, 1.0)\n    \n    async def _evaluate_impact(self, proposal: Proposal, context: EvaluationContext) -> float:\n        \"\"\"Evaluate potential positive impact.\"\"\"\n        score = 0.5  # Base score\n        \n        # Reward improvement keywords\n        improvement_keywords = [\n            'improve', 'optimize', 'enhance', 'better', 'faster', 'efficient'\n        ]\n        \n        for keyword in improvement_keywords:\n            if keyword in proposal.description.lower():\n                score += 0.1\n        \n        # Consider proposal type\n        if proposal.type == 'optimization':\n            score += 0.2\n        elif proposal.type == 'feature':\n            score += 0.15\n        elif proposal.type == 'fix':\n            score += 0.1\n        \n        return min(score, 1.0)\n    \n    async def _evaluate_feasibility(self, proposal: Proposal, context: EvaluationContext) -> float:\n        \"\"\"Evaluate implementation feasibility.\"\"\"\n        score = 0.8  # Base feasibility\n        \n        # Check resource requirements\n        if hasattr(proposal, 'resource_requirements'):\n            for resource, required in proposal.resource_requirements.items():\n                if resource in context.resource_limits:\n                    available = context.resource_limits[resource]\n                    if required > available:\n                        score *= 0.3\n                    elif required > available * 0.8:\n                        score *= 0.7\n        \n        # Consider complexity\n        complexity_indicators = ['complex', 'major', 'significant', 'extensive']\n        for indicator in complexity_indicators:\n            if indicator in proposal.description.lower():\n                score *= 0.8\n        \n        return max(score, 0.1)\n    \n    def _calculate_overall_score(self, safety: float, impact: float, feasibility: float) -> float:\n        \"\"\"Calculate weighted overall score.\"\"\"\n        # Safety is most important, then feasibility, then impact\n        return (safety * 0.5) + (feasibility * 0.3) + (impact * 0.2)\n    \n    def _generate_reasoning(self, safety: float, impact: float, feasibility: float, approved: bool) -> str:\n        \"\"\"Generate human-readable reasoning for the decision.\"\"\"\n        parts = []\n        \n        if safety < self.safety_threshold:\n            parts.append(f\"Safety concerns (score: {safety:.2f})\")\n        if impact < self.impact_threshold:\n            parts.append(f\"Limited impact (score: {impact:.2f})\")\n        if feasibility < 0.6:\n            parts.append(f\"Feasibility issues (score: {feasibility:.2f})\")\n        \n        if approved:\n            return \"Proposal approved: meets all thresholds\"\n        else:\n            return \"Proposal rejected: \" + \", \".join(parts)\n    \n    def _violates_constraint(self, proposal: Proposal, constraint: str) -> bool:\n        \"\"\"Check if proposal violates a safety constraint.\"\"\"\n        # Simple keyword-based constraint checking\n        constraint_lower = constraint.lower()\n        description_lower = proposal.description.lower()\n        \n        if 'no_external_network' in constraint_lower:\n            return any(keyword in description_lower for keyword in \n                      ['network', 'internet', 'http', 'api', 'external'])\n        \n        if 'no_file_system' in constraint_lower:\n            return any(keyword in description_lower for keyword in \n                      ['file', 'directory', 'path', 'write', 'read'])\n        \n        return False\n    \n    def _get_cache_key(self, proposal: Proposal, context: EvaluationContext) -> str:\n        \"\"\"Generate cache key for proposal and context.\"\"\"\n        return f\"{proposal.id}_{hash(str(context.safety_constraints))}\"\n\n\nclass EvaluationWorkflow:\n    \"\"\"Manages the complete evaluation workflow.\"\"\"\n    \n    def __init__(self, evaluator: ProposalEvaluator):\n        \"\"\"Initialize workflow with evaluator.\"\"\"\n        self.evaluator = evaluator\n        self.logger = logging.getLogger(__name__)\n        self._active_evaluations: Dict[str, EvaluationStatus] = {}\n    \n    async def process_proposals(self, proposals: List[Proposal], \n                              context: EvaluationContext) -> List[EvaluationResult]:\n        \"\"\"Process multiple proposals concurrently.\"\"\"\n        results = []\n        \n        # Mark evaluations as in progress\n        for proposal in proposals:\n            self._active_evaluations[proposal.id] = EvaluationStatus.IN_PROGRESS\n        \n        try:\n            # Evaluate all proposals concurrently\n            evaluation_tasks = [\n                self.evaluator.evaluate_proposal(proposal, context)\n                for proposal in proposals\n            ]\n            \n            results = await asyncio.gather(*evaluation_tasks, return_exceptions=True)\n            \n            # Handle any exceptions\n            final_results = []\n            for i, result in enumerate(results):\n                proposal_id = proposals[i].id\n                \n                if isinstance(result, Exception):\n                    self.logger.error(f\"Evaluation failed for {proposal_id}: {result}\")\n                    self._active_evaluations[proposal_id] = EvaluationStatus.FAILED\n                    final_results.append(EvaluationResult(\n                        proposal_id=proposal_id,\n                        approved=False,\n                        safety_score=0.0,\n                        impact_score=0.0,\n                        feasibility_score=0.0,\n                        overall_score=0.0,\n                        reasoning=f\"Evaluation error: {str(result)}\",\n                        timestamp=datetime.now()\n                    ))\n                else:\n                    self._active_evaluations[proposal_id] = EvaluationStatus.COMPLETED\n                    final_results.append(result)\n            \n            return final_results\n            \n        except Exception as e:\n            self.logger.error(f\"Workflow processing failed: {e}\")\n            # Mark all as failed\n            for proposal in proposals:\n                self._active_evaluations[proposal.id] = EvaluationStatus.FAILED\n            raise\n    \n    def get_evaluation_status(self, proposal_id: str) -> Optional[EvaluationStatus]:\n        \"\"\"Get current status of an evaluation.\"\"\"\n        return self._active_evaluations.get(proposal_id)\n    \n    async def batch_evaluate(self, proposals: List[Proposal], \n                           batch_size: int = 10) -> List[EvaluationResult]:\n        \"\"\"Evaluate proposals in batches to manage resources.\"\"\"\n        all_results = []\n        \n        for i in range(0, len(proposals), batch_size):\n            batch = proposals[i:i + batch_size]\n            \n            # Create context for this batch\n            context = EvaluationContext(\n                proposal=batch[0],  # Use first proposal for context\n                current_metrics={},\n                resource_limits={'cpu': 0.8, 'memory': 0.8},\n                safety_constraints=['no_external_network', 'no_destructive_ops'],\n                timestamp=datetime.now()\n            )\n            \n            batch_results = await self.process_proposals(batch, context)\n            all_results.extend(batch_results)\n            \n            # Small delay between batches\n            await asyncio.sleep(0.1)\n        \n        return all_results"
      },
      {
        "path": "swarm/models.py",
        "action": "modify",
        "content": "from enum import Enum\nfrom typing import Dict, Any, Optional, List\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\n\n\nclass ProposalStatus(Enum):\n    \"\"\"Status of a proposal in the evaluation pipeline.\"\"\"\n    PENDING = \"pending\"\n    APPROVED = \"approved\"\n    REJECTED = \"rejected\"\n    IMPLEMENTED = \"implemented\"\n    FAILED = \"failed\"\n\n\n@dataclass\nclass EvaluationCriteria:\n    \"\"\"Criteria used to evaluate proposals.\"\"\"\n    feasibility_weight: float = 0.3\n    impact_weight: float = 0.4\n    risk_weight: float = 0.2\n    alignment_weight: float = 0.1\n    min_score_threshold: float = 0.6\n    \n    def __post_init__(self) -> None:\n        \"\"\"Validate weights sum to 1.0.\"\"\"\n        total_weight = (self.feasibility_weight + self.impact_weight + \n                       self.risk_weight + self.alignment_weight)\n        if abs(total_weight - 1.0) > 0.001:\n            raise ValueError(f\"Weights must sum to 1.0, got {total_weight}\")\n\n\n@dataclass\nclass EvaluationResult:\n    \"\"\"Result of evaluating a proposal.\"\"\"\n    proposal_id: str\n    evaluator_id: str\n    timestamp: datetime = field(default_factory=datetime.now)\n    \n    # Individual scores (0.0 to 1.0)\n    feasibility_score: float = 0.0\n    impact_score: float = 0.0\n    risk_score: float = 0.0  # Lower is better\n    alignment_score: float = 0.0\n    \n    # Overall evaluation\n    weighted_score: float = 0.0\n    decision: ProposalStatus = ProposalStatus.PENDING\n    reasoning: str = \"\"\n    \n    # Additional context\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    \n    def __post_init__(self) -> None:\n        \"\"\"Validate scores are in valid range.\"\"\"\n        scores = [self.feasibility_score, self.impact_score, \n                 self.risk_score, self.alignment_score]\n        for score in scores:\n            if not 0.0 <= score <= 1.0:\n                raise ValueError(f\"Scores must be between 0.0 and 1.0, got {score}\")\n    \n    def calculate_weighted_score(self, criteria: EvaluationCriteria) -> float:\n        \"\"\"Calculate weighted score based on criteria.\"\"\"\n        # Risk score is inverted (lower risk = higher score)\n        adjusted_risk = 1.0 - self.risk_score\n        \n        weighted = (\n            self.feasibility_score * criteria.feasibility_weight +\n            self.impact_score * criteria.impact_weight +\n            adjusted_risk * criteria.risk_weight +\n            self.alignment_score * criteria.alignment_weight\n        )\n        \n        self.weighted_score = weighted\n        return weighted\n    \n    def meets_threshold(self, criteria: EvaluationCriteria) -> bool:\n        \"\"\"Check if evaluation meets minimum threshold.\"\"\"\n        if self.weighted_score == 0.0:\n            self.calculate_weighted_score(criteria)\n        return self.weighted_score >= criteria.min_score_threshold"
      },
      {
        "path": "swarm/orchestrator.py",
        "action": "modify",
        "content": "\"\"\"Orchestrator for the self-improving agent swarm workflow.\"\"\"\n\nimport asyncio\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Any\nfrom enum import Enum\n\nfrom .analyzer import AnalyzerAgent\nfrom .proposer import ProposerAgent\nfrom .evaluator import EvaluatorAgent\nfrom .writer import WriterAgent\nfrom .tester import TesterAgent\n\n\nclass WorkflowState(Enum):\n    \"\"\"Workflow execution states.\"\"\"\n    ANALYZING = \"analyzing\"\n    PROPOSING = \"proposing\"\n    EVALUATING = \"evaluating\"\n    WRITING = \"writing\"\n    TESTING = \"testing\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\n\n@dataclass\nclass WorkflowResult:\n    \"\"\"Result of workflow execution.\"\"\"\n    state: WorkflowState\n    files_changed: List[str]\n    tests_passed: bool\n    error_message: Optional[str] = None\n    metrics: Optional[Dict[str, Any]] = None\n\n\nclass SwarmOrchestrator:\n    \"\"\"Orchestrates the self-improving agent swarm workflow.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the orchestrator with agent instances.\"\"\"\n        self.analyzer = AnalyzerAgent()\n        self.proposer = ProposerAgent()\n        self.evaluator = EvaluatorAgent()\n        self.writer = WriterAgent()\n        self.tester = TesterAgent()\n        self.logger = logging.getLogger(__name__)\n\n    async def execute_improvement_cycle(\n        self, \n        target_area: Optional[str] = None\n    ) -> WorkflowResult:\n        \"\"\"Execute a complete improvement cycle.\"\"\"\n        try:\n            # Phase 1: Analysis\n            self.logger.info(\"Starting analysis phase\")\n            analysis = await self.analyzer.analyze_codebase(target_area)\n            if not analysis.issues:\n                return WorkflowResult(\n                    state=WorkflowState.COMPLETED,\n                    files_changed=[],\n                    tests_passed=True,\n                    metrics={\"issues_found\": 0}\n                )\n\n            # Phase 2: Proposal generation\n            self.logger.info(\"Starting proposal phase\")\n            proposals = await self.proposer.generate_proposals(analysis)\n            if not proposals:\n                return WorkflowResult(\n                    state=WorkflowState.FAILED,\n                    files_changed=[],\n                    tests_passed=False,\n                    error_message=\"No viable proposals generated\"\n                )\n\n            # Phase 3: Evaluation (NEW)\n            self.logger.info(\"Starting evaluation phase\")\n            evaluation = await self.evaluator.evaluate_proposals(proposals)\n            approved_proposals = [p for p in evaluation.proposals if p.approved]\n            \n            if not approved_proposals:\n                return WorkflowResult(\n                    state=WorkflowState.FAILED,\n                    files_changed=[],\n                    tests_passed=False,\n                    error_message=\"No proposals passed evaluation\"\n                )\n\n            # Phase 4: Implementation\n            self.logger.info(\"Starting implementation phase\")\n            implementation = await self.writer.implement_proposals(approved_proposals)\n            \n            # Phase 5: Testing\n            self.logger.info(\"Starting testing phase\")\n            test_result = await self.tester.run_tests(implementation.files_changed)\n\n            return WorkflowResult(\n                state=WorkflowState.COMPLETED if test_result.passed else WorkflowState.FAILED,\n                files_changed=implementation.files_changed,\n                tests_passed=test_result.passed,\n                metrics={\n                    \"issues_found\": len(analysis.issues),\n                    \"proposals_generated\": len(proposals),\n                    \"proposals_approved\": len(approved_proposals),\n                    \"files_modified\": len(implementation.files_changed),\n                    \"tests_run\": test_result.tests_run,\n                    \"evaluation_score\": evaluation.overall_score\n                }\n            )\n\n        except Exception as e:\n            self.logger.error(f\"Workflow execution failed: {e}\")\n            return WorkflowResult(\n                state=WorkflowState.FAILED,\n                files_changed=[],\n                tests_passed=False,\n                error_message=str(e)\n            )\n\n    async def continuous_improvement(\n        self, \n        max_iterations: int = 10,\n        improvement_threshold: float = 0.1\n    ) -> List[WorkflowResult]:\n        \"\"\"Run continuous improvement cycles until convergence.\"\"\"\n        results = []\n        \n        for iteration in range(max_iterations):\n            self.logger.info(f\"Starting improvement iteration {iteration + 1}\")\n            \n            result = await self.execute_improvement_cycle()\n            results.append(result)\n            \n            if result.state == WorkflowState.FAILED:\n                self.logger.warning(f\"Iteration {iteration + 1} failed: {result.error_message}\")\n                break\n                \n            if not result.files_changed:\n                self.logger.info(\"No improvements needed, stopping\")\n                break\n                \n            # Check if improvements are diminishing\n            if (len(results) >= 2 and \n                result.metrics and results[-2].metrics and\n                result.metrics.get(\"issues_found\", 0) > \n                results[-2].metrics.get(\"issues_found\", 0) * (1 - improvement_threshold)):\n                self.logger.info(\"Improvements below threshold, stopping\")\n                break\n                \n            # Brief pause between iterations\n            await asyncio.sleep(1)\n        \n        return results\n\n    def get_workflow_status(self) -> Dict[str, Any]:\n        \"\"\"Get current workflow status and metrics.\"\"\"\n        return {\n            \"agents_active\": 5,\n            \"workflow_phases\": len(WorkflowState),\n            \"evaluation_enabled\": True\n        }"
      }
    ]
  }
}