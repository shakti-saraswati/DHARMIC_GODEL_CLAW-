{
  "id": "evo_20260203_081341_3ff73659",
  "timestamp": "2026-02-03T08:13:41.587656",
  "state": "write",
  "agent": "writer",
  "action": "implementation_complete",
  "parent_id": "evo_20260203_081228_0f217fd4",
  "fitness": null,
  "files_changed": [
    "swarm/utils/__init__.py",
    "swarm/utils/dharmic_reflection.py",
    "swarm/agents/base_agent.py"
  ],
  "metadata": {
    "files": [
      {
        "path": "swarm/utils/__init__.py",
        "action": "create",
        "content": "\"\"\"\nUtilities package for the DHARMIC GODEL CLAW self-improving agent swarm.\n\nThis package provides common utilities and helper functions used across\nthe swarm system.\n\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Union\n\n__version__ = \"0.1.0\"\n__all__ = []\n\n# Package-level constants\nDEFAULT_TIMEOUT = 30.0\nMAX_RETRIES = 3\n\ndef __getattr__(name: str) -> Any:\n    \"\"\"\n    Handle dynamic attribute access for lazy loading of submodules.\n    \n    Args:\n        name: The attribute name being accessed\n        \n    Returns:\n        The requested attribute\n        \n    Raises:\n        AttributeError: If the attribute doesn't exist\n    \"\"\"\n    raise AttributeError(f\"module '{__name__}' has no attribute '{name}'\")"
      },
      {
        "path": "swarm/utils/dharmic_reflection.py",
        "action": "create",
        "content": "from typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DharmicPrinciple(Enum):\n    \"\"\"Core dharmic principles for agent behavior evaluation.\"\"\"\n    HARMONY = \"harmony\"\n    GROWTH = \"growth\"\n    TRUTH = \"truth\"\n    COMPASSION = \"compassion\"\n    WISDOM = \"wisdom\"\n\n\n@dataclass\nclass ReflectionResult:\n    \"\"\"Result of dharmic reflection on an action or decision.\"\"\"\n    overall_score: float\n    principle_scores: Dict[DharmicPrinciple, float]\n    recommendations: List[str]\n    alignment_issues: List[str]\n\n\nclass DharmicReflector:\n    \"\"\"Core dharmic reflection mechanism for evaluating actions and decisions.\"\"\"\n    \n    def __init__(self, principle_weights: Optional[Dict[DharmicPrinciple, float]] = None):\n        \"\"\"Initialize with optional custom principle weights.\"\"\"\n        self.principle_weights = principle_weights or {\n            DharmicPrinciple.HARMONY: 0.2,\n            DharmicPrinciple.GROWTH: 0.2,\n            DharmicPrinciple.TRUTH: 0.2,\n            DharmicPrinciple.COMPASSION: 0.2,\n            DharmicPrinciple.WISDOM: 0.2\n        }\n        self._validate_weights()\n    \n    def _validate_weights(self) -> None:\n        \"\"\"Ensure principle weights sum to 1.0.\"\"\"\n        total = sum(self.principle_weights.values())\n        if abs(total - 1.0) > 0.01:\n            raise ValueError(f\"Principle weights must sum to 1.0, got {total}\")\n    \n    def reflect_on_action(self, action_context: Dict[str, Any]) -> ReflectionResult:\n        \"\"\"Evaluate an action against dharmic principles.\"\"\"\n        try:\n            principle_scores = {}\n            \n            for principle in DharmicPrinciple:\n                score = self._evaluate_principle(principle, action_context)\n                principle_scores[principle] = score\n            \n            overall_score = self._calculate_overall_score(principle_scores)\n            recommendations = self._generate_recommendations(principle_scores, action_context)\n            alignment_issues = self._identify_alignment_issues(principle_scores)\n            \n            return ReflectionResult(\n                overall_score=overall_score,\n                principle_scores=principle_scores,\n                recommendations=recommendations,\n                alignment_issues=alignment_issues\n            )\n        \n        except Exception as e:\n            logger.error(f\"Error during dharmic reflection: {e}\")\n            return self._create_error_result()\n    \n    def _evaluate_principle(self, principle: DharmicPrinciple, context: Dict[str, Any]) -> float:\n        \"\"\"Evaluate a specific principle against the action context.\"\"\"\n        evaluators = {\n            DharmicPrinciple.HARMONY: self._evaluate_harmony,\n            DharmicPrinciple.GROWTH: self._evaluate_growth,\n            DharmicPrinciple.TRUTH: self._evaluate_truth,\n            DharmicPrinciple.COMPASSION: self._evaluate_compassion,\n            DharmicPrinciple.WISDOM: self._evaluate_wisdom\n        }\n        \n        return evaluators[principle](context)\n    \n    def _evaluate_harmony(self, context: Dict[str, Any]) -> float:\n        \"\"\"Evaluate harmony: cooperation, balance, non-interference.\"\"\"\n        score = 0.5  # baseline neutral\n        \n        # Check for cooperative elements\n        if context.get('collaboration', False):\n            score += 0.2\n        if context.get('respects_boundaries', True):\n            score += 0.2\n        else:\n            score -= 0.3\n        \n        # Check for disharmonious elements\n        if context.get('causes_conflict', False):\n            score -= 0.4\n        if context.get('disrupts_systems', False):\n            score -= 0.3\n        \n        return max(0.0, min(1.0, score))\n    \n    def _evaluate_growth(self, context: Dict[str, Any]) -> float:\n        \"\"\"Evaluate growth: learning, improvement, evolution.\"\"\"\n        score = 0.5\n        \n        if context.get('enables_learning', False):\n            score += 0.3\n        if context.get('improves_capabilities', False):\n            score += 0.2\n        if context.get('creates_value', False):\n            score += 0.2\n        \n        if context.get('prevents_growth', False):\n            score -= 0.4\n        if context.get('wastes_resources', False):\n            score -= 0.2\n        \n        return max(0.0, min(1.0, score))\n    \n    def _evaluate_truth(self, context: Dict[str, Any]) -> float:\n        \"\"\"Evaluate truth: accuracy, honesty, transparency.\"\"\"\n        score = 0.5\n        \n        if context.get('is_accurate', True):\n            score += 0.2\n        else:\n            score -= 0.4\n        \n        if context.get('is_transparent', True):\n            score += 0.2\n        else:\n            score -= 0.3\n        \n        if context.get('deceives', False):\n            score -= 0.5\n        if context.get('hides_information', False):\n            score -= 0.2\n        \n        return max(0.0, min(1.0, score))\n    \n    def _evaluate_compassion(self, context: Dict[str, Any]) -> float:\n        \"\"\"Evaluate compassion: consideration for others, harm reduction.\"\"\"\n        score = 0.5\n        \n        if context.get('helps_others', False):\n            score += 0.3\n        if context.get('reduces_harm', True):\n            score += 0.2\n        else:\n            score -= 0.4\n        \n        if context.get('causes_suffering', False):\n            score -= 0.5\n        if context.get('ignores_needs', False):\n            score -= 0.2\n        \n        return max(0.0, min(1.0, score))\n    \n    def _evaluate_wisdom(self, context: Dict[str, Any]) -> float:\n        \"\"\"Evaluate wisdom: good judgment, long-term thinking.\"\"\"\n        score = 0.5\n        \n        if context.get('considers_consequences', True):\n            score += 0.2\n        else:\n            score -= 0.3\n        \n        if context.get('uses_good_judgment', True):\n            score += 0.2\n        if context.get('learns_from_past', True):\n            score += 0.1\n        \n        if context.get('is_reckless', False):\n            score -= 0.4\n        if context.get('ignores_context', False):\n            score -= 0.2\n        \n        return max(0.0, min(1.0, score))\n    \n    def _calculate_overall_score(self, principle_scores: Dict[DharmicPrinciple, float]) -> float:\n        \"\"\"Calculate weighted overall dharmic alignment score.\"\"\"\n        total = sum(\n            self.principle_weights[principle] * score\n            for principle, score in principle_scores.items()\n        )\n        return round(total, 3)\n    \n    def _generate_recommendations(self, principle_scores: Dict[DharmicPrinciple, float], \n                                context: Dict[str, Any]) -> List[str]:\n        \"\"\"Generate improvement recommendations based on principle scores.\"\"\"\n        recommendations = []\n        \n        for principle, score in principle_scores.items():\n            if score < 0.6:\n                recommendations.append(self._get_improvement_suggestion(principle, context))\n        \n        return recommendations\n    \n    def _get_improvement_suggestion(self, principle: DharmicPrinciple, \n                                  context: Dict[str, Any]) -> str:\n        \"\"\"Get specific improvement suggestion for a principle.\"\"\"\n        suggestions = {\n            DharmicPrinciple.HARMONY: \"Consider collaborative approaches and respect for existing systems\",\n            DharmicPrinciple.GROWTH: \"Focus on learning opportunities and value creation\",\n            DharmicPrinciple.TRUTH: \"Ensure accuracy and transparency in communications\",\n            DharmicPrinciple.COMPASSION: \"Consider impact on others and minimize potential harm\",\n            DharmicPrinciple.WISDOM: \"Evaluate long-term consequences and apply good judgment\"\n        }\n        return suggestions[principle]\n    \n    def _identify_alignment_issues(self, principle_scores: Dict[DharmicPrinciple, float]) -> List[str]:\n        \"\"\"Identify significant dharmic alignment issues.\"\"\"\n        issues = []\n        \n        for principle, score in principle_scores.items():\n            if score < 0.3:\n                issues.append(f\"Critical alignment issue with {principle.value}\")\n            elif score < 0.5:\n                issues.append(f\"Moderate alignment concern with {principle.value}\")\n        \n        return issues\n    \n    def _create_error_result(self) -> ReflectionResult:\n        \"\"\"Create a safe default result when reflection fails.\"\"\"\n        return ReflectionResult(\n            overall_score=0.0,\n            principle_scores={p: 0.0 for p in DharmicPrinciple},\n            recommendations=[\"Unable to complete dharmic reflection - review action context\"],\n            alignment_issues=[\"Reflection process failed\"]\n        )"
      },
      {
        "path": "swarm/agents/base_agent.py",
        "action": "modify",
        "content": "from abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional, List\nimport logging\nfrom datetime import datetime\nimport json\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseAgent(ABC):\n    \"\"\"Base class for all agents in the DHARMIC GODEL CLAW swarm.\"\"\"\n    \n    def __init__(self, agent_id: str, config: Dict[str, Any]):\n        \"\"\"Initialize base agent.\n        \n        Args:\n            agent_id: Unique identifier for this agent\n            config: Configuration dictionary\n        \"\"\"\n        self.agent_id = agent_id\n        self.config = config\n        self.checkpoints: List[Dict[str, Any]] = []\n        self.current_state: Dict[str, Any] = {}\n        \n    @abstractmethod\n    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Process input and return output.\n        \n        Args:\n            input_data: Input data to process\n            \n        Returns:\n            Processed output data\n        \"\"\"\n        pass\n    \n    def dharmic_reflection(self, action: str, outcome: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Perform dharmic reflection on an action and its outcome.\n        \n        Args:\n            action: Description of the action taken\n            outcome: Results and consequences of the action\n            \n        Returns:\n            Reflection analysis with insights and adjustments\n        \"\"\"\n        try:\n            reflection = {\n                \"timestamp\": datetime.utcnow().isoformat(),\n                \"agent_id\": self.agent_id,\n                \"action\": action,\n                \"outcome\": outcome,\n                \"analysis\": self._analyze_dharmic_alignment(action, outcome),\n                \"adjustments\": self._generate_adjustments(action, outcome)\n            }\n            \n            self._save_checkpoint(\"dharmic_reflection\", reflection)\n            logger.info(f\"Agent {self.agent_id} completed dharmic reflection on: {action}\")\n            \n            return reflection\n            \n        except Exception as e:\n            logger.error(f\"Error in dharmic reflection for agent {self.agent_id}: {e}\")\n            return {\n                \"error\": str(e),\n                \"timestamp\": datetime.utcnow().isoformat(),\n                \"agent_id\": self.agent_id\n            }\n    \n    def _analyze_dharmic_alignment(self, action: str, outcome: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Analyze how well an action aligns with dharmic principles.\n        \n        Args:\n            action: Action taken\n            outcome: Outcome of the action\n            \n        Returns:\n            Analysis of dharmic alignment\n        \"\"\"\n        principles = [\"ahimsa\", \"truthfulness\", \"non_attachment\", \"service\"]\n        alignment_scores = {}\n        \n        for principle in principles:\n            alignment_scores[principle] = self._score_principle_alignment(\n                principle, action, outcome\n            )\n        \n        return {\n            \"principle_scores\": alignment_scores,\n            \"overall_alignment\": sum(alignment_scores.values()) / len(alignment_scores),\n            \"recommendations\": self._generate_dharmic_recommendations(alignment_scores)\n        }\n    \n    def _score_principle_alignment(self, principle: str, action: str, outcome: Dict[str, Any]) -> float:\n        \"\"\"Score alignment with a specific dharmic principle.\n        \n        Args:\n            principle: Dharmic principle to evaluate\n            action: Action taken\n            outcome: Outcome of the action\n            \n        Returns:\n            Alignment score between 0.0 and 1.0\n        \"\"\"\n        # Base implementation - can be overridden by specific agents\n        base_score = 0.7\n        \n        # Adjust based on outcome success\n        if outcome.get(\"success\", False):\n            base_score += 0.1\n        \n        # Adjust based on harm/benefit indicators\n        if outcome.get(\"harm_indicators\", []):\n            base_score -= 0.2\n        \n        if outcome.get(\"benefit_indicators\", []):\n            base_score += 0.1\n            \n        return max(0.0, min(1.0, base_score))\n    \n    def _generate_dharmic_recommendations(self, alignment_scores: Dict[str, float]) -> List[str]:\n        \"\"\"Generate recommendations based on dharmic alignment scores.\n        \n        Args:\n            alignment_scores: Scores for each dharmic principle\n            \n        Returns:\n            List of recommendations for improvement\n        \"\"\"\n        recommendations = []\n        \n        for principle, score in alignment_scores.items():\n            if score < 0.6:\n                recommendations.append(f\"Improve alignment with {principle}\")\n            elif score > 0.9:\n                recommendations.append(f\"Maintain excellent {principle} practices\")\n                \n        if not recommendations:\n            recommendations.append(\"Continue current dharmic practices\")\n            \n        return recommendations\n    \n    def _generate_adjustments(self, action: str, outcome: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Generate behavioral adjustments based on reflection.\n        \n        Args:\n            action: Action that was taken\n            outcome: Results of the action\n            \n        Returns:\n            Suggested adjustments for future actions\n        \"\"\"\n        adjustments = {\n            \"behavioral_changes\": [],\n            \"parameter_updates\": {},\n            \"process_improvements\": []\n        }\n        \n        # Add adjustments based on outcome analysis\n        if not outcome.get(\"success\", True):\n            adjustments[\"behavioral_changes\"].append(\"Increase validation before action\")\n            \n        if outcome.get(\"efficiency\", 1.0) < 0.8:\n            adjustments[\"process_improvements\"].append(\"Optimize processing pipeline\")\n            \n        return adjustments\n    \n    def _save_checkpoint(self, checkpoint_type: str, data: Dict[str, Any]) -> None:\n        \"\"\"Save a checkpoint of agent state.\n        \n        Args:\n            checkpoint_type: Type of checkpoint being saved\n            data: Data to save in checkpoint\n        \"\"\"\n        checkpoint = {\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"agent_id\": self.agent_id,\n            \"type\": checkpoint_type,\n            \"data\": data,\n            \"state_snapshot\": self.current_state.copy()\n        }\n        \n        self.checkpoints.append(checkpoint)\n        \n        # Keep only last 10 checkpoints to manage memory\n        if len(self.checkpoints) > 10:\n            self.checkpoints = self.checkpoints[-10:]\n    \n    def get_checkpoints(self, checkpoint_type: Optional[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve checkpoints, optionally filtered by type.\n        \n        Args:\n            checkpoint_type: Optional type filter\n            \n        Returns:\n            List of matching checkpoints\n        \"\"\"\n        if checkpoint_type is None:\n            return self.checkpoints.copy()\n        \n        return [cp for cp in self.checkpoints if cp.get(\"type\") == checkpoint_type]\n    \n    def restore_checkpoint(self, checkpoint_index: int) -> bool:\n        \"\"\"Restore agent state from a checkpoint.\n        \n        Args:\n            checkpoint_index: Index of checkpoint to restore\n            \n        Returns:\n            True if restoration successful, False otherwise\n        \"\"\"\n        try:\n            if 0 <= checkpoint_index < len(self.checkpoints):\n                checkpoint = self.checkpoints[checkpoint_index]\n                self.current_state = checkpoint[\"data\"][\"state_snapshot\"].copy()\n                logger.info(f\"Agent {self.agent_id} restored from checkpoint {checkpoint_index}\")\n                return True\n            return False\n            \n        except Exception as e:\n            logger.error(f\"Error restoring checkpoint for agent {self.agent_id}: {e}\")\n            return False\n    \n    def update_state(self, state_updates: Dict[str, Any]) -> None:\n        \"\"\"Update agent's current state.\n        \n        Args:\n            state_updates: Dictionary of state updates to apply\n        \"\"\"\n        self.current_state.update(state_updates)\n        self._save_checkpoint(\"state_update\", {\"updates\": state_updates})\n    \n    def get_state(self) -> Dict[str, Any]:\n        \"\"\"Get current agent state.\n        \n        Returns:\n            Current state dictionary\n        \"\"\"\n        return self.current_state.copy()"
      }
    ]
  }
}