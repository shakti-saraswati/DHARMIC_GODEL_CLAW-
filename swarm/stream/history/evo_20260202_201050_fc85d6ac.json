{
  "id": "evo_20260202_201050_fc85d6ac",
  "timestamp": "2026-02-02T20:10:50.132339",
  "state": "write",
  "agent": "writer",
  "action": "implementation_complete",
  "parent_id": "evo_20260202_200920_17c8aaf3",
  "fitness": null,
  "files_changed": [
    "swarm/utils/__init__.py",
    "swarm/utils/fitness_predictor.py",
    "swarm/utils/dharmic_impact_model.py"
  ],
  "metadata": {
    "files": [
      {
        "path": "swarm/utils/__init__.py",
        "action": "create",
        "content": "\"\"\"Utils package for the DHARMIC GODEL CLAW agent swarm.\n\nThis package provides utility functions and classes used across the swarm system.\n\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Union\nimport logging\n\n__version__ = \"0.1.0\"\n__all__ = [\n    \"setup_logging\",\n    \"validate_config\",\n    \"format_error\",\n]\n\nlogger = logging.getLogger(__name__)\n\n\ndef setup_logging(level: str = \"INFO\", format_string: Optional[str] = None) -> None:\n    \"\"\"Set up logging configuration for the swarm.\n    \n    Args:\n        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n        format_string: Custom format string for log messages\n        \n    Raises:\n        ValueError: If level is invalid\n    \"\"\"\n    valid_levels = {\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"}\n    if level.upper() not in valid_levels:\n        raise ValueError(f\"Invalid log level: {level}. Must be one of {valid_levels}\")\n    \n    if format_string is None:\n        format_string = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    \n    logging.basicConfig(\n        level=getattr(logging, level.upper()),\n        format=format_string,\n        datefmt=\"%Y-%m-%d %H:%M:%S\"\n    )\n    logger.info(f\"Logging initialized at {level} level\")\n\n\ndef validate_config(config: Dict[str, Any], required_keys: List[str]) -> bool:\n    \"\"\"Validate that a configuration dictionary contains required keys.\n    \n    Args:\n        config: Configuration dictionary to validate\n        required_keys: List of required key names\n        \n    Returns:\n        True if all required keys are present\n        \n    Raises:\n        KeyError: If any required key is missing\n        TypeError: If config is not a dictionary\n    \"\"\"\n    if not isinstance(config, dict):\n        raise TypeError(\"Config must be a dictionary\")\n    \n    missing_keys = [key for key in required_keys if key not in config]\n    if missing_keys:\n        raise KeyError(f\"Missing required config keys: {missing_keys}\")\n    \n    logger.debug(f\"Config validation passed for keys: {required_keys}\")\n    return True\n\n\ndef format_error(error: Exception, context: Optional[str] = None) -> str:\n    \"\"\"Format an error message with optional context.\n    \n    Args:\n        error: The exception to format\n        context: Optional context string to include\n        \n    Returns:\n        Formatted error message string\n    \"\"\"\n    error_type = type(error).__name__\n    error_msg = str(error)\n    \n    if context:\n        return f\"{context}: {error_type} - {error_msg}\"\n    else:\n        return f\"{error_type}: {error_msg}\""
      },
      {
        "path": "swarm/utils/fitness_predictor.py",
        "action": "create",
        "content": "\"\"\"Fitness prediction engine with simulation capabilities.\"\"\"\n\nimport logging\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nlogger = logging.getLogger(__name__)\n\n\nclass FitnessMetric(Enum):\n    \"\"\"Types of fitness metrics that can be predicted.\"\"\"\n    PERFORMANCE = \"performance\"\n    EFFICIENCY = \"efficiency\"\n    RELIABILITY = \"reliability\"\n    ADAPTABILITY = \"adaptability\"\n    RESOURCE_USAGE = \"resource_usage\"\n\n\n@dataclass\nclass FitnessData:\n    \"\"\"Container for fitness measurement data.\"\"\"\n    metric: FitnessMetric\n    value: float\n    timestamp: datetime\n    context: Dict[str, Any]\n    confidence: float = 1.0\n\n\n@dataclass\nclass SimulationResult:\n    \"\"\"Result of a fitness simulation.\"\"\"\n    predicted_fitness: float\n    confidence: float\n    factors: Dict[str, float]\n    simulation_time: float\n    iterations: int\n\n\nclass FitnessPredictor:\n    \"\"\"Core fitness prediction engine with simulation capabilities.\"\"\"\n    \n    def __init__(self, history_window: int = 100):\n        \"\"\"Initialize fitness predictor.\n        \n        Args:\n            history_window: Number of historical data points to consider\n        \"\"\"\n        self.history_window = history_window\n        self.fitness_history: List[FitnessData] = []\n        self.baseline_metrics: Dict[FitnessMetric, float] = {}\n        \n    def record_fitness(self, data: FitnessData) -> None:\n        \"\"\"Record a fitness measurement.\n        \n        Args:\n            data: Fitness measurement to record\n        \"\"\"\n        try:\n            self.fitness_history.append(data)\n            \n            # Maintain history window\n            if len(self.fitness_history) > self.history_window:\n                self.fitness_history = self.fitness_history[-self.history_window:]\n                \n            # Update baseline if this is the first measurement for this metric\n            if data.metric not in self.baseline_metrics:\n                self.baseline_metrics[data.metric] = data.value\n                \n        except Exception as e:\n            logger.error(f\"Failed to record fitness data: {e}\")\n            raise\n    \n    def predict_fitness(self, \n                       metric: FitnessMetric,\n                       context: Dict[str, Any],\n                       horizon: int = 10) -> float:\n        \"\"\"Predict fitness for given metric and context.\n        \n        Args:\n            metric: Type of fitness metric to predict\n            context: Environmental/operational context\n            horizon: Prediction horizon (time steps ahead)\n            \n        Returns:\n            Predicted fitness value\n        \"\"\"\n        try:\n            relevant_history = [\n                d for d in self.fitness_history \n                if d.metric == metric\n            ]\n            \n            if not relevant_history:\n                return self.baseline_metrics.get(metric, 0.5)\n            \n            # Simple trend-based prediction\n            recent_values = [d.value for d in relevant_history[-10:]]\n            if len(recent_values) < 2:\n                return recent_values[0] if recent_values else 0.5\n            \n            # Calculate trend\n            trend = (recent_values[-1] - recent_values[0]) / len(recent_values)\n            base_value = recent_values[-1]\n            \n            # Apply context adjustments\n            context_factor = self._calculate_context_factor(context, metric)\n            \n            predicted = base_value + (trend * horizon * context_factor)\n            return max(0.0, min(1.0, predicted))\n            \n        except Exception as e:\n            logger.error(f\"Fitness prediction failed: {e}\")\n            return 0.5\n    \n    def simulate_scenario(self,\n                         scenario: Dict[str, Any],\n                         metrics: List[FitnessMetric],\n                         iterations: int = 1000) -> SimulationResult:\n        \"\"\"Simulate fitness under a given scenario.\n        \n        Args:\n            scenario: Scenario parameters to simulate\n            metrics: Fitness metrics to evaluate\n            iterations: Number of simulation iterations\n            \n        Returns:\n            Simulation results\n        \"\"\"\n        start_time = datetime.now()\n        \n        try:\n            total_fitness = 0.0\n            factor_sums: Dict[str, float] = {}\n            \n            for i in range(iterations):\n                iteration_fitness = 0.0\n                \n                for metric in metrics:\n                    # Add some randomness to simulation\n                    noise = np.random.normal(0, 0.1)\n                    predicted = self.predict_fitness(metric, scenario)\n                    adjusted = max(0.0, min(1.0, predicted + noise))\n                    \n                    iteration_fitness += adjusted\n                    \n                    # Track factors\n                    factor_key = f\"{metric.value}_impact\"\n                    if factor_key not in factor_sums:\n                        factor_sums[factor_key] = 0.0\n                    factor_sums[factor_key] += adjusted\n                \n                total_fitness += iteration_fitness / len(metrics)\n            \n            # Calculate averages\n            avg_fitness = total_fitness / iterations\n            avg_factors = {k: v / iterations for k, v in factor_sums.items()}\n            \n            # Calculate confidence based on variance\n            confidence = self._calculate_confidence(metrics, scenario)\n            \n            simulation_time = (datetime.now() - start_time).total_seconds()\n            \n            return SimulationResult(\n                predicted_fitness=avg_fitness,\n                confidence=confidence,\n                factors=avg_factors,\n                simulation_time=simulation_time,\n                iterations=iterations\n            )\n            \n        except Exception as e:\n            logger.error(f\"Simulation failed: {e}\")\n            return SimulationResult(\n                predicted_fitness=0.5,\n                confidence=0.0,\n                factors={},\n                simulation_time=0.0,\n                iterations=0\n            )\n    \n    def get_fitness_trend(self, \n                         metric: FitnessMetric,\n                         window: int = 20) -> Tuple[float, str]:\n        \"\"\"Get fitness trend for a specific metric.\n        \n        Args:\n            metric: Fitness metric to analyze\n            window: Number of recent data points to consider\n            \n        Returns:\n            Tuple of (trend_slope, trend_direction)\n        \"\"\"\n        try:\n            relevant_history = [\n                d for d in self.fitness_history[-window:]\n                if d.metric == metric\n            ]\n            \n            if len(relevant_history) < 2:\n                return 0.0, \"stable\"\n            \n            values = [d.value for d in relevant_history]\n            x = np.arange(len(values))\n            \n            # Simple linear regression\n            slope = np.polyfit(x, values, 1)[0]\n            \n            if slope > 0.01:\n                direction = \"improving\"\n            elif slope < -0.01:\n                direction = \"declining\"\n            else:\n                direction = \"stable\"\n                \n            return slope, direction\n            \n        except Exception as e:\n            logger.error(f\"Trend analysis failed: {e}\")\n            return 0.0, \"unknown\"\n    \n    def _calculate_context_factor(self, \n                                 context: Dict[str, Any],\n                                 metric: FitnessMetric) -> float:\n        \"\"\"Calculate context adjustment factor for predictions.\"\"\"\n        try:\n            factor = 1.0\n            \n            # Resource availability impact\n            if \"resource_availability\" in context:\n                resource_level = context[\"resource_availability\"]\n                if metric == FitnessMetric.PERFORMANCE:\n                    factor *= (0.5 + 0.5 * resource_level)\n                elif metric == FitnessMetric.RESOURCE_USAGE:\n                    factor *= (1.5 - 0.5 * resource_level)\n            \n            # Load level impact\n            if \"system_load\" in context:\n                load = context[\"system_load\"]\n                if metric in [FitnessMetric.PERFORMANCE, FitnessMetric.RELIABILITY]:\n                    factor *= (1.2 - 0.4 * load)\n            \n            # Environmental stability\n            if \"environment_stability\" in context:\n                stability = context[\"environment_stability\"]\n                if metric == FitnessMetric.ADAPTABILITY:\n                    factor *= (1.0 + 0.3 * (1.0 - stability))\n            \n            return max(0.1, min(2.0, factor))\n            \n        except Exception as e:\n            logger.warning(f\"Context factor calculation failed: {e}\")\n            return 1.0\n    \n    def _calculate_confidence(self,\n                            metrics: List[FitnessMetric],\n                            scenario: Dict[str, Any]) -> float:\n        \"\"\"Calculate confidence in prediction based on historical data.\"\"\"\n        try:\n            if not self.fitness_history:\n                return 0.1\n            \n            # Base confidence on amount of historical data\n            data_confidence = min(1.0, len(self.fitness_history) / 50.0)\n            \n            # Reduce confidence for metrics with high variance\n            variance_penalty = 0.0\n            for metric in metrics:\n                metric_values = [\n                    d.value for d in self.fitness_history \n                    if d.metric == metric\n                ]\n                if len(metric_values) > 1:\n                    variance = np.var(metric_values)\n                    variance_penalty += variance * 0.1\n            \n            confidence = data_confidence * (1.0 - min(0.8, variance_penalty))\n            return max(0.1, confidence)\n            \n        except Exception as e:\n            logger.warning(f\"Confidence calculation failed: {e}\")\n            return 0.5"
      },
      {
        "path": "swarm/utils/dharmic_impact_model.py",
        "action": "create",
        "content": "from typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DharmicPrinciple(Enum):\n    \"\"\"Core dharmic principles for evaluation.\"\"\"\n    TRUTH = \"truth\"\n    NON_HARM = \"non_harm\"\n    COMPASSION = \"compassion\"\n    WISDOM = \"wisdom\"\n    BALANCE = \"balance\"\n    GROWTH = \"growth\"\n    RESPONSIBILITY = \"responsibility\"\n\n\n@dataclass\nclass DharmicScore:\n    \"\"\"Represents a dharmic evaluation score.\"\"\"\n    principle: DharmicPrinciple\n    score: float  # -1.0 to 1.0\n    confidence: float  # 0.0 to 1.0\n    reasoning: str\n\n\n@dataclass\nclass DharmicEvaluation:\n    \"\"\"Complete dharmic evaluation of a proposal.\"\"\"\n    overall_score: float\n    principle_scores: List[DharmicScore]\n    recommendation: str\n    concerns: List[str]\n    benefits: List[str]\n\n\nclass DharmicImpactModel:\n    \"\"\"Model for evaluating dharmic alignment of proposals.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the dharmic impact model.\"\"\"\n        self.principle_weights = {\n            DharmicPrinciple.TRUTH: 0.20,\n            DharmicPrinciple.NON_HARM: 0.25,\n            DharmicPrinciple.COMPASSION: 0.15,\n            DharmicPrinciple.WISDOM: 0.15,\n            DharmicPrinciple.BALANCE: 0.10,\n            DharmicPrinciple.GROWTH: 0.10,\n            DharmicPrinciple.RESPONSIBILITY: 0.05\n        }\n    \n    def evaluate_proposal(self, proposal: Dict) -> DharmicEvaluation:\n        \"\"\"Evaluate a proposal against dharmic principles.\"\"\"\n        try:\n            principle_scores = []\n            \n            for principle in DharmicPrinciple:\n                score = self._evaluate_principle(proposal, principle)\n                principle_scores.append(score)\n            \n            overall_score = self._calculate_overall_score(principle_scores)\n            recommendation = self._generate_recommendation(overall_score)\n            concerns = self._identify_concerns(principle_scores)\n            benefits = self._identify_benefits(principle_scores)\n            \n            return DharmicEvaluation(\n                overall_score=overall_score,\n                principle_scores=principle_scores,\n                recommendation=recommendation,\n                concerns=concerns,\n                benefits=benefits\n            )\n            \n        except Exception as e:\n            logger.error(f\"Error evaluating proposal: {e}\")\n            return self._create_error_evaluation()\n    \n    def _evaluate_principle(self, proposal: Dict, principle: DharmicPrinciple) -> DharmicScore:\n        \"\"\"Evaluate a proposal against a specific dharmic principle.\"\"\"\n        proposal_type = proposal.get('type', '')\n        description = proposal.get('description', '')\n        impact = proposal.get('impact', '')\n        \n        if principle == DharmicPrinciple.TRUTH:\n            return self._evaluate_truth(proposal_type, description, impact)\n        elif principle == DharmicPrinciple.NON_HARM:\n            return self._evaluate_non_harm(proposal_type, description, impact)\n        elif principle == DharmicPrinciple.COMPASSION:\n            return self._evaluate_compassion(proposal_type, description, impact)\n        elif principle == DharmicPrinciple.WISDOM:\n            return self._evaluate_wisdom(proposal_type, description, impact)\n        elif principle == DharmicPrinciple.BALANCE:\n            return self._evaluate_balance(proposal_type, description, impact)\n        elif principle == DharmicPrinciple.GROWTH:\n            return self._evaluate_growth(proposal_type, description, impact)\n        elif principle == DharmicPrinciple.RESPONSIBILITY:\n            return self._evaluate_responsibility(proposal_type, description, impact)\n        else:\n            return DharmicScore(principle, 0.0, 0.0, \"Unknown principle\")\n    \n    def _evaluate_truth(self, proposal_type: str, description: str, impact: str) -> DharmicScore:\n        \"\"\"Evaluate truthfulness and transparency.\"\"\"\n        score = 0.5  # Neutral baseline\n        confidence = 0.7\n        reasoning = \"Evaluating truthfulness and transparency\"\n        \n        # Positive indicators\n        if any(word in description.lower() for word in ['transparent', 'honest', 'clear', 'accurate']):\n            score += 0.3\n        if 'documentation' in description.lower():\n            score += 0.2\n        \n        # Negative indicators\n        if any(word in description.lower() for word in ['hidden', 'obscure', 'misleading']):\n            score -= 0.4\n        \n        return DharmicScore(DharmicPrinciple.TRUTH, min(1.0, max(-1.0, score)), confidence, reasoning)\n    \n    def _evaluate_non_harm(self, proposal_type: str, description: str, impact: str) -> DharmicScore:\n        \"\"\"Evaluate potential for harm.\"\"\"\n        score = 0.3  # Slightly positive baseline\n        confidence = 0.8\n        reasoning = \"Evaluating potential for harm\"\n        \n        # Positive indicators\n        if any(word in description.lower() for word in ['safe', 'secure', 'protect', 'careful']):\n            score += 0.3\n        if 'test' in description.lower() or 'validation' in description.lower():\n            score += 0.2\n        \n        # Negative indicators\n        if any(word in description.lower() for word in ['risk', 'danger', 'break', 'delete']):\n            score -= 0.4\n        if 'experimental' in description.lower():\n            score -= 0.2\n        \n        return DharmicScore(DharmicPrinciple.NON_HARM, min(1.0, max(-1.0, score)), confidence, reasoning)\n    \n    def _evaluate_compassion(self, proposal_type: str, description: str, impact: str) -> DharmicScore:\n        \"\"\"Evaluate compassionate intent.\"\"\"\n        score = 0.2  # Slightly positive baseline\n        confidence = 0.6\n        reasoning = \"Evaluating compassionate intent\"\n        \n        # Positive indicators\n        if any(word in description.lower() for word in ['help', 'improve', 'benefit', 'support']):\n            score += 0.3\n        if any(word in description.lower() for word in ['user', 'community', 'accessibility']):\n            score += 0.2\n        \n        return DharmicScore(DharmicPrinciple.COMPASSION, min(1.0, max(-1.0, score)), confidence, reasoning)\n    \n    def _evaluate_wisdom(self, proposal_type: str, description: str, impact: str) -> DharmicScore:\n        \"\"\"Evaluate wisdom and thoughtfulness.\"\"\"\n        score = 0.1  # Neutral baseline\n        confidence = 0.7\n        reasoning = \"Evaluating wisdom and thoughtfulness\"\n        \n        # Positive indicators\n        if any(word in description.lower() for word in ['consider', 'analyze', 'research', 'study']):\n            score += 0.3\n        if any(word in description.lower() for word in ['best practice', 'proven', 'established']):\n            score += 0.2\n        \n        # Negative indicators\n        if any(word in description.lower() for word in ['quick', 'hack', 'workaround']):\n            score -= 0.2\n        \n        return DharmicScore(DharmicPrinciple.WISDOM, min(1.0, max(-1.0, score)), confidence, reasoning)\n    \n    def _evaluate_balance(self, proposal_type: str, description: str, impact: str) -> DharmicScore:\n        \"\"\"Evaluate balance and moderation.\"\"\"\n        score = 0.2  # Slightly positive baseline\n        confidence = 0.5\n        reasoning = \"Evaluating balance and moderation\"\n        \n        # Positive indicators\n        if any(word in description.lower() for word in ['balance', 'moderate', 'gradual', 'incremental']):\n            score += 0.3\n        \n        # Negative indicators\n        if any(word in description.lower() for word in ['extreme', 'radical', 'massive']):\n            score -= 0.3\n        \n        return DharmicScore(DharmicPrinciple.BALANCE, min(1.0, max(-1.0, score)), confidence, reasoning)\n    \n    def _evaluate_growth(self, proposal_type: str, description: str, impact: str) -> DharmicScore:\n        \"\"\"Evaluate potential for positive growth.\"\"\"\n        score = 0.3  # Positive baseline\n        confidence = 0.6\n        reasoning = \"Evaluating potential for positive growth\"\n        \n        # Positive indicators\n        if any(word in description.lower() for word in ['enhance', 'improve', 'optimize', 'expand']):\n            score += 0.3\n        if any(word in description.lower() for word in ['learn', 'adapt', 'evolve']):\n            score += 0.2\n        \n        return DharmicScore(DharmicPrinciple.GROWTH, min(1.0, max(-1.0, score)), confidence, reasoning)\n    \n    def _evaluate_responsibility(self, proposal_type: str, description: str, impact: str) -> DharmicScore:\n        \"\"\"Evaluate responsibility and accountability.\"\"\"\n        score = 0.2  # Slightly positive baseline\n        confidence = 0.6\n        reasoning = \"Evaluating responsibility and accountability\"\n        \n        # Positive indicators\n        if any(word in description.lower() for word in ['responsible', 'accountable', 'maintain']):\n            score += 0.3\n        if any(word in description.lower() for word in ['monitor', 'track', 'measure']):\n            score += 0.2\n        \n        return DharmicScore(DharmicPrinciple.RESPONSIBILITY, min(1.0, max(-1.0, score)), confidence, reasoning)\n    \n    def _calculate_overall_score(self, principle_scores: List[DharmicScore]) -> float:\n        \"\"\"Calculate weighted overall dharmic score.\"\"\"\n        total_score = 0.0\n        total_weight = 0.0\n        \n        for score in principle_scores:\n            weight = self.principle_weights.get(score.principle, 0.0)\n            total_score += score.score * weight * score.confidence\n            total_weight += weight * score.confidence\n        \n        return total_score / total_weight if total_weight > 0 else 0.0\n    \n    def _generate_recommendation(self, overall_score: float) -> str:\n        \"\"\"Generate recommendation based on overall score.\"\"\"\n        if overall_score >= 0.6:\n            return \"APPROVE\"\n        elif overall_score >= 0.2:\n            return \"APPROVE_WITH_CAUTION\"\n        elif overall_score >= -0.2:\n            return \"NEEDS_REVISION\"\n        else:\n            return \"REJECT\"\n    \n    def _identify_concerns(self, principle_scores: List[DharmicScore]) -> List[str]:\n        \"\"\"Identify dharmic concerns from principle scores.\"\"\"\n        concerns = []\n        \n        for score in principle_scores:\n            if score.score < -0.2:\n                concerns.append(f\"Low {score.principle.value} score: {score.reasoning}\")\n        \n        return concerns\n    \n    def _identify_benefits(self, principle_scores: List[DharmicScore]) -> List[str]:\n        \"\"\"Identify dharmic benefits from principle scores.\"\"\"\n        benefits = []\n        \n        for score in principle_scores:\n            if score.score > 0.4:\n                benefits.append(f\"Strong {score.principle.value} alignment: {score.reasoning}\")\n        \n        return benefits\n    \n    def _create_error_evaluation(self) -> DharmicEvaluation:\n        \"\"\"Create a neutral evaluation for error cases.\"\"\"\n        return DharmicEvaluation(\n            overall_score=0.0,\n            principle_scores=[],\n            recommendation=\"NEEDS_MANUAL_REVIEW\",\n            concerns=[\"Error during evaluation\"],\n            benefits=[]\n        )"
      }
    ]
  }
}